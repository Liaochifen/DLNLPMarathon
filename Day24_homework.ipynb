{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFHEqlFxzJnd"
   },
   "source": [
    "# 作業 : 實做向量拼接方式 ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrohU0VUzR4j"
   },
   "source": [
    "# [作業目標]\n",
    "- 實做向量拼接方式 ATTENTION\n",
    "- 運用 實做的 ATTENTION FUNCTION 在之前的 RNN seq 2 seq attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD016DwpzSZb"
   },
   "source": [
    "# [作業重點]\n",
    "向量拼接方式 ATTENTION\n",
    "- 先將 $q$ and $k$ concat 起來\n",
    "- 然後經過一層 $W$ attention 自學參數,\n",
    "- 和一個 $tanh$ activation function. \n",
    "- 最後乘以一個 $V_t$ 調整成一個同等於input seq 的數列.\n",
    "$$\n",
    "R(q,k)=v^Ttanh(W[q;k])\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWKMMKlb0nHE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fslSk8lOzFt5"
   },
   "outputs": [],
   "source": [
    "# 請在這邊實做向量拼接方式 ATTENTION \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_emb_dim):\n",
    "        super().__init__()\n",
    "        # (enc_hid_dim * 2) is from k, dec_emb_dim is from q\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_emb_dim, dec_emb_dim)\n",
    "        self.v = nn.Linear(dec_emb_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_input, encoder_outputs, mask):\n",
    "        # dec_input = [1, bz, dec_emb_dim]\n",
    "        # encoder_outputs = [src len, bz, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # repeat dec_input state src_len times\n",
    "        # 這邊開始，同學可以跟隨我們的建議完成程式或是自行寫作\n",
    "        # 整理代表 q and k 的 dec_input and encoder_output \n",
    "        dec_input = dec_input.permute(1,0,2).expand(batch_size, src_len, dec_input.size(2))\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        # dec_input = [bz, src len, dec_emb_dim]\n",
    "        # encoder_outputs = [bz, src len, enc hid dim * 2]\n",
    "        \n",
    "        # 計算 向量拼接方式 ATTENTION\n",
    "        # 先將 q and k concat 起來\n",
    "        # 然後經過一層 W attention 自學參數,\n",
    "        # 和一個 tanh activation function.\n",
    "        # 最後乘以一個 Vt 調整成一個同等於input seq 的數列.\n",
    "        attention = torch.tanh(self.attn(torch.cat([encoder_outputs, dec_input], dim=-1)))\n",
    "        # attention = [bz, src len, dec_emb_dim]\n",
    "        \n",
    "        # 將 ATTENTION 結果乘以 V\n",
    "        attention = torch.squeeze(self.v(attention), dim=-1)\n",
    "        # attention = [bz, src len, 1] -> [bz, src len]\n",
    "        \n",
    "        # apply MASK 建議使用 tensor 的 masked_fill \n",
    "        attention = attention.masked_fill(mask==0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+oRm4MxVZ+mOIEGj+nMkQ",
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
