{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrWWUhFlUprI"
   },
   "source": [
    "# 作業 : 實作英文-德文翻譯機器人\n",
    "***\n",
    "## [作業目標]\n",
    "\n",
    "用 pytorch 實作一個英文-德文翻譯機器人\n",
    "\n",
    "## [作業目標]\n",
    "\n",
    "*   語言資料處理\n",
    "*   使用 LSTM 建構 Encoder: EncoderLSTM\n",
    "*   使用 LSTM 建構 Decoder: DecoderLSTM\n",
    "*   搭建 Sequence to Sequence 模型: Seq2Seq\n",
    "*   撰寫訓練函式\n",
    "*   撰寫測試函式\n",
    "\n",
    "## [問題]\n",
    "\n",
    "在 Colab 實際上執行完這個範例後，請改用 BiLSTM 來建構 Encoder 與 Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgPfcYR26SxF"
   },
   "source": [
    "## 安裝 spacy\n",
    "\n",
    "We'll also make use of spaCy to tokenize our data. To install spaCy, follow the instructions here making sure to install both the English and German models with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22976,
     "status": "ok",
     "timestamp": 1619681502820,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "Lzjl5xnFTm6l",
    "outputId": "fac7756b-2096-4bd4-c65c-798b3006688f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling spacy-2.2.4:\n",
      "  Successfully uninstalled spacy-2.2.4\n",
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 201kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Collecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 54.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 46.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 51.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 59.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=b1337241e79b044174c2a27fffc34ac45b0a2c130c8e07605359936cfb36f2cf\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: typer, catalogue, spacy-legacy, smart-open, pathy, srsly, pydantic, thinc, spacy\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "Successfully installed catalogue-2.0.3 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall spacy -y\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12694,
     "status": "ok",
     "timestamp": 1619681584187,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "kNl9AK7PiZqq",
    "outputId": "dfe3b967-b6c5-4092-bfeb-86fff29dd29d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-29 07:32:55.154439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Requirement already satisfied: de-core-news-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl#egg=de_core_news_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (56.0.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "2021-04-29 07:33:01.062098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (56.0.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyd4LijE7vGo"
   },
   "source": [
    "## 引用需要的模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1619691627993,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "RanKHsWTu-rn"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9h9rmgr74Sk"
   },
   "source": [
    "## 下載英文及德文語料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1333,
     "status": "ok",
     "timestamp": 1619681592341,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "RxcQmEiziZqs",
    "outputId": "1334c48c-483b-46cb-9406-f5bd900b03b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.de.gz: 100%|██████████| 637k/637k [00:00<00:00, 21.8MB/s]\n",
      "train.en.gz: 100%|██████████| 569k/569k [00:00<00:00, 21.8MB/s]\n",
      "val.de.gz: 100%|██████████| 24.7k/24.7k [00:00<00:00, 6.44MB/s]\n",
      "val.en.gz: 100%|██████████| 21.6k/21.6k [00:00<00:00, 5.96MB/s]\n",
      "test_2016_flickr.de.gz: 100%|██████████| 22.9k/22.9k [00:00<00:00, 5.49MB/s]\n",
      "test_2016_flickr.en.gz: 100%|██████████| 21.1k/21.1k [00:00<00:00, 4.97MB/s]\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5874,
     "status": "ok",
     "timestamp": 1619681601136,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "iscq_mByiZqs"
   },
   "outputs": [],
   "source": [
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3336,
     "status": "ok",
     "timestamp": 1619681609066,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "G25546yRiZqt"
   },
   "outputs": [],
   "source": [
    "raw_en_data = []\n",
    "raw_de_data = []\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        raw_de = raw_de.strip()\n",
    "        raw_en = raw_en.strip()\n",
    "        raw_en_data.append(raw_en)\n",
    "        raw_de_data.append(raw_de)\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                                dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                                dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1619681611488,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "3u6B-ZFhiZqt",
    "outputId": "0ed809a6-122e-4f5e-d2ff-a04a4bd5b023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'machine', 'learning']\n",
      "Unique tokens in source (german) vocabulary: 19215\n",
      "Unique tokens in target (en) vocabulary: 10838\n"
     ]
    }
   ],
   "source": [
    "def tokenize_de(text):\n",
    "    return [token for token in de_tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token for token in en_tokenizer(text)]\n",
    "\n",
    "### Sample Run ###\n",
    "sample_text = \"I love machine learning\"\n",
    "print(tokenize_english(sample_text))\n",
    "\n",
    "print(f\"Unique tokens in source (german) vocabulary: {len(de_vocab.stoi)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab.stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1619681614617,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "SBB83WbiiZqu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1619681617148,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "mdYv5pdbiZqu",
    "outputId": "4c1c49d0-3acc-4904-9b58-59e2d9a29210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "German example: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche .\n",
      "English example: Two young , White males are outside near many bushes .\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(val_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "german = \" \".join([de_vocab.itos[i] for i in train_data[0][0]])\n",
    "english = \" \".join([en_vocab.itos[i] for i in train_data[0][1]])\n",
    "\n",
    "print(f\"German example: {german}\")\n",
    "print(f\"English example: {english}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1619681620203,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "DYKWFN8IiZqv",
    "outputId": "fd31dc82-e1c0-4963-c840-1b85a4eb956c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German -  Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche .  Length -  13\n",
      "English -  Two young , White males are outside near many bushes .  Length -  11\n",
      "\n",
      "German -  Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem .  Length -  8\n",
      "English -  Several men in hard hats are operating a giant pulley system .  Length -  12\n",
      "\n",
      "German -  Ein kleines Mädchen klettert in ein Spielhaus aus Holz .  Length -  10\n",
      "English -  A little girl climbing into a wooden playhouse .  Length -  9\n",
      "\n",
      "German -  Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .  Length -  15\n",
      "English -  A man in a blue shirt is standing on a ladder cleaning a window .  Length -  15\n",
      "\n",
      "German -  Zwei Männer stehen am Herd und bereiten Essen zu .  Length -  10\n",
      "English -  Two men are at the stove preparing food .  Length -  9\n",
      "\n",
      "German -  Ein Mann in grün hält eine Gitarre , während der andere Mann sein Hemd ansieht .  Length -  16\n",
      "English -  A man in green holds a guitar while the other man observes his shirt .  Length -  15\n",
      "\n",
      "German -  Ein Mann lächelt einen ausgestopften Löwen an .  Length -  8\n",
      "English -  A man is smiling at a stuffed lion  Length -  8\n",
      "\n",
      "German -  Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt .  Length -  14\n",
      "English -  A trendy girl talking on her cellphone while gliding slowly down the street .  Length -  14\n",
      "\n",
      "German -  Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei .  Length -  12\n",
      "English -  A woman with a large purse is walking by a gate .  Length -  12\n",
      "\n",
      "German -  Jungen tanzen mitten in der Nacht auf Pfosten .  Length -  9\n",
      "English -  Boys dancing on poles in the middle of the night .  Length -  11\n",
      "\n",
      "Maximum Length of English sentence 41 and German sentence 44 in the dataset\n",
      "Minimum Length of English sentence 4 and German sentence 1 in the dataset\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "max_len_eng = []\n",
    "max_len_ger = []\n",
    "for german_seq, en_seq in train_data:\n",
    "    german_sentence = \" \".join([de_vocab.itos[i] for i in german_seq])\n",
    "    en_sentence = \" \".join([en_vocab.itos[i] for i in en_seq])\n",
    "    if count < 10 :\n",
    "        print(\"German - \",german_sentence, \" Length - \", len(german_seq))\n",
    "        print(\"English - \",en_sentence, \" Length - \", len(en_seq))\n",
    "        print()\n",
    "    count += 1\n",
    "\n",
    "# train_data = [(german_sentence, english_sentence), ...]\n",
    "en_max_length = len(max(train_data, key=lambda pair: len(pair[1]))[1])\n",
    "en_min_length = len(min(train_data, key=lambda pair: len(pair[1]))[1])\n",
    "german_max_length = len(max(train_data, key=lambda pair: len(pair[0]))[0])\n",
    "german_min_length = len(min(train_data, key=lambda pair: len(pair[0]))[0])\n",
    "\n",
    "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(en_max_length, german_max_length))\n",
    "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(en_min_length, german_min_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLV-t-MzVQdg"
   },
   "source": [
    "## 用 LSTM 搭建的 Encoder 類別: EncoderLSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1619681624048,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "0dZT3Zs17yMQ"
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "\n",
    "        # Size of the one hot vectors that will be the input to the encoder\n",
    "        #self.input_size = input_size\n",
    "\n",
    "        # Output size of the word embedding NN\n",
    "        #self.embedding_size = embedding_size\n",
    "\n",
    "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Number of layers in the lstm\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.tag = True\n",
    "\n",
    "        # Shape --------------------> (19219, 300) [input size, embedding dims]\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Shape -----------> (300, 1024, 2) [embedding dims, hidden size, num layers]\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "\n",
    "    # Shape of x (26, 128) [Sequence_length, batch_size]\n",
    "    def forward(self, x):\n",
    "        # Shape -----------> (26, 128, 300) [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # Shape --> outputs (26, 128, 1024) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) (2, 128, 1024) , (2, 128, 1024) [num_layers, batch_size size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTew1tbHVer5"
   },
   "source": [
    "## 用 LSTM 搭建的 decoder 類別: DecoderLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1619681627692,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "wPGbQiBP72iX"
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        # Size of the one hot vectors that will be the input to the encoder\n",
    "        #self.input_size = input_size\n",
    "\n",
    "        # Output size of the word embedding NN\n",
    "        #self.embedding_size = embedding_size\n",
    "\n",
    "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Number of layers in the lstm\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # Shape --------------------> (10838, 300) [input size, embedding dims]\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Shape -----------> (300, 1024, 2) [embedding dims, hidden size, num layers]\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "\n",
    "        # Shape -----------> (1024, 10838) [hidden size, output size]\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Shape of x (128) [batch_size]\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # Shape of x (1, 128) [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Shape -----------> (1, 128, 300) [1, batch_size, embedding dims]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # Shape --> outputs (1, 128, 1024) [1, batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) (2, 128, 1024) , (2, 128, 1024) [num_layers, batch_size size, hidden_size] \n",
    "        # (passing encoder's hs, cs - context vectors)\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "\n",
    "        # Shape --> predictions (1, 128, 10838) [ 1, batch_size , output_size]\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # Shape --> predictions (128, 10838) [batch_size , output_size]\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11962,
     "status": "ok",
     "timestamp": 1619681641317,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "1UDj1yCsiZqx",
    "outputId": "c4d5006b-b1bd-46c3-9bc4-3bb4364a7f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: EncoderLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(19215, 300)\n",
      "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      ") \n",
      "\n",
      "Decoder: DecoderLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(10838, 300)\n",
      "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      "  (fc): Linear(in_features=1024, out_features=10838, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_input_size = len(de_vocab.stoi)\n",
    "encoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "\n",
    "encoder_lstm = EncoderLSTM(encoder_input_size, encoder_embedding_size,\n",
    "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
    "\n",
    "# Decoder\n",
    "decoder_input_size = len(en_vocab.stoi)\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "decoder_dropout = 0.5\n",
    "output_size = len(en_vocab.stoi)\n",
    "\n",
    "decoder_lstm = DecoderLSTM(decoder_input_size, decoder_embedding_size,\n",
    "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
    "\n",
    "print(\"Encoder:\", encoder_lstm, \"\\n\")\n",
    "print(\"Decoder:\", decoder_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGnQbCnGVire"
   },
   "source": [
    "# Sequence to Sequence 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1619681644472,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "_vzOor_Q782h"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "\n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        # Shape - Source : (10, 128) [(Sentence length german + some padding), Number of Sentences]\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        # Shape - Source : (14, 128) [(Sentence length English + some padding), Number of Sentences]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(en_vocab.itos)\n",
    "\n",
    "        # Shape --> outputs (14, 128, 10838) \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        # Shape --> (hs, cs) (2, 128, 1024) ,(2, 128, 1024) [num_layers, batch_size size, hidden_size] \n",
    "        # (contains encoder's hs, cs - context vectors)\n",
    "        hidden_state, cell_state = self.Encoder_LSTM(source)\n",
    "\n",
    "        # [<bos> * 128] (128 elements)\n",
    "        x = target[0] # Trigger token <bos>\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            # Shape --> output (128, 10838) \n",
    "            output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "        # Shape --> outputs (14, 128, 10838) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1619681646720,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "ywW6f9fM8AMa"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = en_vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1619681649130,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "yD0pRilG8CHJ",
    "outputId": "533bb075-4ba9-4e24-b566-2a95810f2372"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (Encoder_LSTM): EncoderLSTM(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(19215, 300)\n",
       "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (Decoder_LSTM): DecoderLSTM(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(10838, 300)\n",
       "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=1024, out_features=10838, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1161,
     "status": "ok",
     "timestamp": 1619691443444,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "fQyZ_vfq8G6C"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [german.itos[idx] for idx in sentence]\n",
    "    text_to_indices = [german.stoi['<bos>']] + [german.stoi[token] for token in tokens] + [german.stoi['<eos>']]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
    "\n",
    "    outputs = [english.stoi[\"<bos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if best_guess == english.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "# 用來評估模型的函式: bleu\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = example[0]\n",
    "        trg = [english.itos[idx] for idx in example[1][1:]]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        if len(prediction) == 49 and prediction[-1] == english.stoi['<eos>']:\n",
    "            prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append(trg)\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
    "    print('saving')\n",
    "    print()\n",
    "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
    "    torch.save(state, './checkpoint-NMT')\n",
    "    torch.save(model.state_dict(),'./checkpoint-NMT-SD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9727312,
     "status": "error",
     "timestamp": 1619691418919,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "ysc4A5HX8Qyg",
    "outputId": "3308178b-b78a-422e-a653-ac3d479d2c5d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'a', 'a', 'a', 'a', '.', '.', '.', '<eos>']\n",
      "saving\n",
      "\n",
      "Epoch_Loss - 4.6899285316467285\n",
      "\n",
      "Epoch - 2 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 4.134108543395996\n",
      "\n",
      "Epoch - 3 / 100\n",
      "Translated example sentence 1: \n",
      " ['The', 'in', 'a', 'red', 'and', 'and', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.983480930328369\n",
      "\n",
      "Epoch - 4 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'in', 'a', 'a', ',', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.8228628635406494\n",
      "\n",
      "Epoch - 5 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.5762898921966553\n",
      "\n",
      "Epoch - 6 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'worker', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.3658337593078613\n",
      "\n",
      "Epoch - 7 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'person', 'is', 'on', 'a', 'stage', 'and', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.4067864418029785\n",
      "\n",
      "Epoch - 8 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 3.2461094856262207\n",
      "\n",
      "Epoch - 9 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.80832839012146\n",
      "\n",
      "Epoch - 10 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.9930596351623535\n",
      "\n",
      "Epoch - 11 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'a', 'a', 'standing', 'on', 'a', 'a', 'and', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.74168062210083\n",
      "\n",
      "Epoch - 12 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'and', 'blue', 'standing', 'on', 'a', 'a', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.470717430114746\n",
      "\n",
      "Epoch - 13 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'a', 'blue', 'standing', 'on', 'a', 'stage', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.633047342300415\n",
      "\n",
      "Epoch - 14 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'and', 'blue', 'standing', 'on', 'a', 'stage', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.370054006576538\n",
      "\n",
      "Epoch - 15 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', ',', 'standing', 'on', 'a', 'stage', 'and', 'a', 'a', '.', '.', '<eos>']\n",
      "Epoch_Loss - 2.2915139198303223\n",
      "\n",
      "Epoch - 16 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', ',', 'standing', 'in', 'a', 'a', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 2.237182855606079\n",
      "\n",
      "Epoch - 17 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'stage', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.8800958395004272\n",
      "\n",
      "Epoch - 18 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'stand', 'on', 'a', 'stage', 'and', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.9805819988250732\n",
      "\n",
      "Epoch - 19 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'stage', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.737153172492981\n",
      "\n",
      "Epoch - 20 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'stage', 'looking', 'at', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.7794204950332642\n",
      "\n",
      "Epoch - 21 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'sidewalk', 'and', 'a', 'a', 'tiger', '.', '<eos>']\n",
      "Epoch_Loss - 1.7518545389175415\n",
      "\n",
      "Epoch - 22 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'platform', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.588564157485962\n",
      "\n",
      "Epoch - 23 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'standing', 'standing', 'on', 'a', 'blanket', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.6184117794036865\n",
      "\n",
      "Epoch - 24 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'standing', 'standing', 'on', 'a', 'large', 'platform', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.8201156854629517\n",
      "\n",
      "Epoch - 25 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'blue', 'platform', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.5590757131576538\n",
      "\n",
      "Epoch - 26 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'standing', 'on', 'a', 'platform', 'platform', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.5894775390625\n",
      "\n",
      "Epoch - 27 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'platform', 'and', 'a', 'a', 'tiger', '.', '<eos>']\n",
      "Epoch_Loss - 1.4194804430007935\n",
      "\n",
      "Epoch - 28 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'blue', 'standing', 'on', 'a', 'large', 'platform', 'and', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.4607818126678467\n",
      "\n",
      "Epoch - 29 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'blue', 'standing', 'in', 'a', 'large', 'platform', 'while', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.478317379951477\n",
      "\n",
      "Epoch - 30 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'standing', 'in', 'a', 'blue', 'plastic', '-', 'up', 'up', 'and', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.3933076858520508\n",
      "\n",
      "Epoch - 31 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'standing', 'on', 'a', 'blue', 'blue', 'and', 'blue', 'balloon', 'and', 'a', 'large', 'screen', '.', '<eos>']\n",
      "Epoch_Loss - 1.3987343311309814\n",
      "\n",
      "Epoch - 32 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'rock', 'in', 'blue', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'pole', '.', '<eos>']\n",
      "Epoch_Loss - 1.1771758794784546\n",
      "\n",
      "Epoch - 33 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'a', 'blue', 'blue', 'blue', 'blue', 'stand', 'on', 'a', 'large', 'pole', '.', '<eos>']\n",
      "Epoch_Loss - 1.2392196655273438\n",
      "\n",
      "Epoch - 34 / 100\n",
      "Translated example sentence 1: \n",
      " ['African', 'American', 'football', 'blue', 'standing', 'on', 'a', 'large', 'pole', 'and', 'pointing', 'to', 'a', 'cluttered', '.', '<eos>']\n",
      "Epoch_Loss - 0.9921493530273438\n",
      "\n",
      "Epoch - 35 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'pole', 'and', 'pointing', 'to', 'a', 'camera', '.', '<eos>']\n",
      "Epoch_Loss - 1.0554274320602417\n",
      "\n",
      "Epoch - 36 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'rock', 'in', 'blue', 'blue', 'standing', 'standing', 'on', 'a', 'pole', 'in', 'a', 'large', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.9237508177757263\n",
      "\n",
      "Epoch - 37 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'a', 'blue', 'blue', 'sweatshirt', 'standing', 'on', 'a', 'pole', 'looking', 'pointing', 'at', 'a', 'yellow', 'sign', '.', '<eos>']\n",
      "Epoch_Loss - 0.8551983833312988\n",
      "\n",
      "Epoch - 38 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'sweatshirt', 'standing', 'on', 'a', 'pole', 'pole', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 1.1517651081085205\n",
      "\n",
      "Epoch - 39 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'in', 'a', 'blue', 'blue', 'blue', 'blanket', 'standing', 'on', 'a', 'pole', 'and', 'a', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.9843870997428894\n",
      "\n",
      "Epoch - 40 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'inside', 'of', 'a', 'blue', 'blue', 'stand', 'and', 'a', 'a', 'pole', '.', '<eos>']\n",
      "Epoch_Loss - 0.9075921177864075\n",
      "\n",
      "Epoch - 41 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'scouts', 'in', 'blue', 'blue', 'standing', 'standing', 'on', 'a', 'large', 'pole', 'and', 'a', 'walkie', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.8643872141838074\n",
      "\n",
      "Epoch - 42 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'blue', 'standing', 'standing', 'on', 'a', 'pole', 'looking', 'at', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6333771347999573\n",
      "\n",
      "Epoch - 43 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'line', 'standing', 'standing', 'in', 'a', 'large', 'pole', 'pointing', 'to', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.8219858407974243\n",
      "\n",
      "Epoch - 44 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'standing', 'standing', 'on', 'a', 'large', 'ledge', 'and', 'a', 'large', 'screen', '.', '<eos>']\n",
      "Epoch_Loss - 0.8780707716941833\n",
      "\n",
      "Epoch - 45 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'standing', 'standing', 'standing', 'in', 'a', 'large', 'pole', 'and', 'a', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6940529942512512\n",
      "\n",
      "Epoch - 46 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'wearing', 'blue', 'blue', 'tank', 'suit', 'standing', 'on', 'a', 'pole', 'and', 'a', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6794019341468811\n",
      "\n",
      "Epoch - 47 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'blue', 'shirt', 'standing', 'on', 'a', 'stool', 'looking', 'at', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6516454815864563\n",
      "\n",
      "Epoch - 48 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'tank', 'top', 'standing', 'on', 'a', 'stool', 'and', 'throwing', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6922029852867126\n",
      "\n",
      "Epoch - 49 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'is', 'standing', 'in', 'a', 'blue', 'blue', 'piece', 'of', 'a', 'child', 'standing', 'a', 'small', 'platform', '.', '<eos>']\n",
      "Epoch_Loss - 0.4966750741004944\n",
      "\n",
      "Epoch - 50 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'pole', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 0.5345053672790527\n",
      "\n",
      "Epoch - 51 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'workers', 'in', 'a', 'blue', 'blue', 'blue', 'stand', 'stand', 'on', 'a', 'pole', '.', '<eos>']\n",
      "Epoch_Loss - 0.7656895518302917\n",
      "\n",
      "Epoch - 52 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'suit', 'standing', 'on', 'a', 'stool', 'and', 'a', 'a', 'camera', '.', '<eos>']\n",
      "Epoch_Loss - 0.5768231153488159\n",
      "\n",
      "Epoch - 53 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'suit', 'standing', 'on', 'a', 'pole', 'and', 'a', 'a', 'safety', 'sign', '.', '<eos>']\n",
      "Epoch_Loss - 0.3644830286502838\n",
      "\n",
      "Epoch - 54 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'stool', 'while', 'a', 'a', 'light', 'costume', '.', '<eos>']\n",
      "Epoch_Loss - 0.6730818748474121\n",
      "\n",
      "Epoch - 55 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'is', 'wearing', 'blue', 'blue', 'blue', 'standing', 'on', 'a', 'pole', 'and', 'looks', 'at', 'a', 'yellow', '.', '<eos>']\n",
      "Epoch_Loss - 0.5032026767730713\n",
      "\n",
      "Epoch - 56 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'large', 'pole', 'and', 'a', 'yellow', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.3557957112789154\n",
      "\n",
      "Epoch - 57 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'is', 'standing', 'in', 'a', 'blue', 'blue', 'chair', 'while', 'pointing', 'at', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.6948075294494629\n",
      "\n",
      "Epoch - 58 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'is', 'standing', 'on', 'a', 'blue', 'blue', 'piece', 'of', 'equipment', 'equipment', 'a', 'book', '.', '<eos>']\n",
      "Epoch_Loss - 0.5741561651229858\n",
      "\n",
      "Epoch - 59 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'standing', 'standing', 'in', 'a', 'rain', 'and', 'throwing', 'a', 'screen', '.', '<eos>']\n",
      "Epoch_Loss - 0.42504358291625977\n",
      "\n",
      "Epoch - 60 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'is', 'blue', 'blue', 'standing', 'on', 'standing', 'in', 'a', 'large', 'piece', 'of', 'musical', 'chairs', '.', '<eos>']\n",
      "Epoch_Loss - 0.659694492816925\n",
      "\n",
      "Epoch - 61 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'tank', 'top', 'standing', 'on', 'a', 'stool', 'and', 'pointing', 'to', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.40886759757995605\n",
      "\n",
      "Epoch - 62 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'blue', 'standing', 'on', 'a', 'ladder', 'throwing', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.31579282879829407\n",
      "\n",
      "Epoch - 63 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'blue', 'standing', 'on', 'a', 'stool', 'looking', 'a', 'pointing', '.', '<eos>']\n",
      "Epoch_Loss - 0.36335259675979614\n",
      "\n",
      "Epoch - 64 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'suit', 'standing', 'on', 'a', 'ladder', 'and', 'pointing', 'to', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.3267368972301483\n",
      "\n",
      "Epoch - 65 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'suit', 'standing', 'on', 'a', 'stool', 'and', 'throwing', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.33850619196891785\n",
      "\n",
      "Epoch - 66 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'man', 'standing', 'on', 'a', 'blue', 'blue', 'piece', 'of', 'blue', 'equipment', 'and', 'a', 'of', 'a', '<eos>']\n",
      "Epoch_Loss - 0.33222582936286926\n",
      "\n",
      "Epoch - 67 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'pole', 'looking', 'pointing', 'a', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.2906327247619629\n",
      "\n",
      "Epoch - 68 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'table', 'throwing', 'a', 'toy', '.', '<eos>']\n",
      "Epoch_Loss - 0.28926193714141846\n",
      "\n",
      "Epoch - 69 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'standing', 'in', 'a', 'stand', 'and', 'throwing', 'a', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.2063998132944107\n",
      "\n",
      "Epoch - 70 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'in', 'a', 'blue', 'blue', 'standing', 'standing', 'on', 'a', 'pole', 'looking', 'at', 'a', 'yellow', 'light', '.', '<eos>']\n",
      "Epoch_Loss - 0.25332263112068176\n",
      "\n",
      "Epoch - 71 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'blue', 'blue', 'standing', 'standing', 'up', 'a', 'pole', 'standing', 'in', 'a', 'line', '.', '<eos>']\n",
      "Epoch_Loss - 0.29801541566848755\n",
      "\n",
      "Epoch - 72 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'standing', 'on', 'a', 'large', 'pole', 'and', 'slide', '.', '<eos>']\n",
      "Epoch_Loss - 0.3521170914173126\n",
      "\n",
      "Epoch - 73 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'blue', 'dress', 'standing', 'up', 'a', 'a', 'chalkboard', '.', '<eos>']\n",
      "Epoch_Loss - 0.30378425121307373\n",
      "\n",
      "Epoch - 74 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'workers', 'in', 'blue', 'blue', 'standing', 'on', 'a', 'rope', 'while', 'throwing', 'a', 'yellow', 'line', '.', '<eos>']\n",
      "Epoch_Loss - 0.31973031163215637\n",
      "\n",
      "Epoch - 75 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'knee', 'stand', 'on', 'a', 'large', 'pole', 'and', 'one', 'basketball', '.', '<eos>']\n",
      "Epoch_Loss - 0.25799745321273804\n",
      "\n",
      "Epoch - 76 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'in', 'a', 'blue', 'standing', 'up', 'on', 'a', 'large', 'platform', 'and', 'a', '.', '<eos>']\n",
      "Epoch_Loss - 0.21300852298736572\n",
      "\n",
      "Epoch - 77 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'knee', 'blue', 'knee', 'while', 'standing', 'on', 'a', 'stage', '.', '<eos>']\n",
      "Epoch_Loss - 0.23969708383083344\n",
      "\n",
      "Epoch - 78 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'blue', 'dress', 'standing', 'on', 'a', 'large', 'drum', '.', '<eos>']\n",
      "Epoch_Loss - 0.29242759943008423\n",
      "\n",
      "Epoch - 79 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'tank', 'blue', 'jeans', 'stand', 'on', 'a', 'large', 'staircase', '.', '<eos>']\n",
      "Epoch_Loss - 0.2631482481956482\n",
      "\n",
      "Epoch - 80 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'in', 'a', 'blue', 'standing', 'up', 'on', 'a', 'pole', 'and', 'a', 'a', 'light', 'costume', '.', '<eos>']\n",
      "Epoch_Loss - 0.2807557284832001\n",
      "\n",
      "Epoch - 81 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'in', 'a', 'blue', 'standing', 'on', 'a', 'large', 'drum', 'and', 'a', 'a', 'plume', 'of', 'of', 'the', '.', '<eos>']\n",
      "Epoch_Loss - 0.21284860372543335\n",
      "\n",
      "Epoch - 82 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'up', 'on', 'standing', 'in', 'a', 'large', 'wooden', 'cloth', '.', '<eos>']\n",
      "Epoch_Loss - 0.2074384093284607\n",
      "\n",
      "Epoch - 83 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'kid', 'standing', 'in', 'a', 'blue', 'blue', 'playing', 'a', 'large', 'blue', 'and', 'yellow', 'court', '.', '<eos>']\n",
      "Epoch_Loss - 0.25402891635894775\n",
      "\n",
      "Epoch - 84 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'kid', 'in', 'a', 'blue', 'blue', 'standing', 'on', 'a', 'blue', 'piece', 'of', 'flowers', 'and', 'a', 'toy', '.', '<eos>']\n",
      "Epoch_Loss - 0.1788918823003769\n",
      "\n",
      "Epoch - 85 / 100\n",
      "Translated example sentence 1: \n",
      " ['Street', 'man', 'standing', 'in', 'a', 'blue', 'blue', 'dress', 'standing', 'up', 'pointing', 'a', 'drum', '.', '<eos>']\n",
      "Epoch_Loss - 0.23852388560771942\n",
      "\n",
      "Epoch - 86 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'blue', 'stand', 'on', 'standing', 'in', 'a', 'large', 'blue', 'slide', '.', '<eos>']\n",
      "Epoch_Loss - 0.3002646565437317\n",
      "\n",
      "Epoch - 87 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'standing', 'in', 'a', 'blue', 'blanket', 'on', 'a', 'blue', 'harness', 'and', 'one', 'holds', 'a', 'costume', '.', '<eos>']\n",
      "Epoch_Loss - 0.21495197713375092\n",
      "\n",
      "Epoch - 88 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'blue', 'blue', 'standing', 'on', 'a', 'blue', 'slide', 'and', 'throwing', 'a', 'yellow', '.', '<eos>']\n",
      "Epoch_Loss - 0.11142396181821823\n",
      "\n",
      "Epoch - 89 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'in', 'blue', 'blue', 'standing', 'on', 'a', 'rope', 'in', 'a', 'large', 'show', '<eos>']\n",
      "Epoch_Loss - 0.2987103760242462\n",
      "\n",
      "Epoch - 90 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'in', 'a', 'blue', 'blue', 'suit', 'standing', 'together', 'on', 'a', 'stool', '.', '<eos>']\n",
      "Epoch_Loss - 0.139980286359787\n",
      "\n",
      "Epoch - 91 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'in', 'a', 'blue', 'blue', 'playing', 'a', 'a', 'a', '<eos>']\n",
      "Epoch_Loss - 0.17714041471481323\n",
      "\n",
      "Epoch - 92 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'in', 'a', 'blue', 'playing', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch_Loss - 0.1391003131866455\n",
      "\n",
      "Epoch - 93 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'man', 'standing', 'in', 'a', 'blue', 'blue', 'sweatshirt', ',', 'pointing', 'by', 'a', 'blue', 'sign', '.', '<eos>']\n",
      "Epoch_Loss - 0.20921891927719116\n",
      "\n",
      "Epoch - 94 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'in', 'a', 'blue', 'blue', 'playing', 'a', 'a', 'a', 'harness', '.', '<eos>']\n",
      "Epoch_Loss - 0.28324684500694275\n",
      "\n",
      "Epoch - 95 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'kid', 'standing', 'in', 'a', 'blue', 'blue', 'blue', 'outfit', 'while', 'standing', 'on', 'a', 'amplifier', '.', '<eos>']\n",
      "Epoch_Loss - 0.2559555470943451\n",
      "\n",
      "Epoch - 96 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'kid', 'in', 'blue', 'standing', 'on', 'standing', 'in', 'a', 'large', 'wooden', 'village', '.', '<eos>']\n",
      "Epoch_Loss - 0.1912333071231842\n",
      "\n",
      "Epoch - 97 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'band', 'is', 'standing', 'underneath', 'a', 'blue', 'blue', 'looking', 'in', 'a', 'rain', '.', '<eos>']\n",
      "Epoch_Loss - 0.1408369243144989\n",
      "\n",
      "Epoch - 98 / 100\n",
      "Translated example sentence 1: \n",
      " ['A', 'kid', 'standing', 'in', 'a', 'blue', 'blue', 'blanket', ',', 'smiling', 'at', 'a', 'son', '.', '<eos>']\n",
      "Epoch_Loss - 0.1613859236240387\n",
      "\n",
      "Epoch - 99 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'standing', 'up', 'a', 'blue', 'slide', 'and', 'a', 'in', 'a', 'large', '<eos>']\n",
      "Epoch_Loss - 0.13025034964084625\n",
      "\n",
      "Epoch - 100 / 100\n",
      "Translated example sentence 1: \n",
      " ['Two', 'workers', 'standing', 'in', 'a', 'blue', 'place', 'on', 'a', 'blue', 'and', 'a', 'a', 'light', 'costume', '.', '<eos>']\n",
      "Epoch_Loss - 0.22881799936294556\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b7bdf7c0e530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# score = bleu(test_data[1:100], model, german, english, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0.0\n",
    "num_epochs = 100\n",
    "best_loss = 999999\n",
    "best_epoch = -1\n",
    "sentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
    "ts1  = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "    model.eval()\n",
    "    translated_sentence1 = translate_sentence(model, sentence1, de_vocab, en_vocab, device, max_length=50)\n",
    "    print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
    "    ts1.append(translated_sentence1)\n",
    "\n",
    "    model.train(True)\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        input_german = batch[0].to(device)\n",
    "        target_en = batch[1].to(device)\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input_german, target_en)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target_en = target_en[1:].reshape(-1)\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target_en)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epoch_loss = 0.0\n",
    "        best_epoch = epoch\n",
    "        checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
    "        if ((epoch - best_epoch) >= 10):\n",
    "            print(\"no improvement in 10 epochs, break\")\n",
    "            break\n",
    "    print(\"Epoch_Loss - {}\".format(loss.item()))\n",
    "    print()\n",
    "    \n",
    "print(epoch_loss / len(train_iter))\n",
    "\n",
    "score = bleu(test_data[:100], model, de_vocab, en_vocab, device)\n",
    "print(f\"Bleu score {score*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17367,
     "status": "ok",
     "timestamp": 1619691656586,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "-qfYB5DAHKMw",
    "outputId": "dba917a7-b3de-45c0-acdd-830f24460360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.08028613762839\n",
      "Bleu score 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(epoch_loss / len(train_iter))\n",
    "\n",
    "score = bleu(test_data[:10], model, de_vocab, en_vocab, device)\n",
    "print(f\"Bleu score {score*100:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day22_作業_Seq2Seq_translator_en_de.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
