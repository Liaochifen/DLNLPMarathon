{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liaochifen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/liaochifen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "with open(\"data/aclImdb/imdb.vocab\", \"r\") as f:\n",
    "    vocab = f.readlines()\n",
    "    vocab = [word.strip() for word in vocab]\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "\n",
    "stop_words = set(stopwords.words('English'))\n",
    "vocab = set(vocab).difference(stop_words)\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data/aclImdb/train/pos/4715_9.txt', 1), ('data/aclImdb/train/pos/12390_8.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "train_pos_review_file_paths = glob.glob(\"data/aclImdb/train/pos/*\")\n",
    "train_neg_review_file_paths = glob.glob(\"data/aclImdb/train/neg/*\")\n",
    "train_review_file_paths = train_pos_review_file_paths + train_neg_review_file_paths\n",
    "labels = [1]*len(train_pos_review_file_paths) + [0]*len(train_neg_review_file_paths)\n",
    "\n",
    "review_pairs = list(zip(train_review_file_paths, labels))\n",
    "\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    with open(review_path, \"r\") as f:\n",
    "        review = f.read().lower()\n",
    "        review = re.sub(r\"<.*?>\", \"\", review)\n",
    "        review = re.sub(r\"[^a-zA-Z]\", \" \", review)\n",
    "        \n",
    "    token_with_stop_words = nltk.word_tokenize(review)\n",
    "    token_no_stop_words = [token for token in token_with_stop_words if token not in stop_words]\n",
    " \n",
    "    return token_no_stop_words\n",
    "    \n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    vector = []\n",
    "    for token in review:\n",
    "        if vocab_dic.get(token):\n",
    "            vector.append(vocab_dic[token])\n",
    "    \n",
    "    return torch.tensor(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: dict\n",
    "        dict of {vocabulary: idx}\n",
    "    '''\n",
    "    def __init__(self, data_pairs, vocab):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        token = load_review(self.data_pairs[idx][0])\n",
    "        vector = generate_vec(token, self.vocab)\n",
    "        label = torch.tensor(self.data_pairs[idx][1])\n",
    "        \n",
    "        return vector.clone().detach(), label.clone().detach()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "    vectors, labels = zip(*batch)\n",
    "    lengths = [len(vector) for vector in vectors]\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    sentence_batch = []\n",
    "    for vector in vectors:\n",
    "        temp_pad = torch.zeros(max_length)\n",
    "        temp_pad[:len(vector)] = vector\n",
    "        sentence_batch.append(temp_pad.reshape(-1, max_length))\n",
    "    \n",
    "    return torch.cat(sentence_batch, dim=0), torch.tensor(labels), torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[34621., 66692., 73972., 47393., 68549., 40473., 62083., 82164.,   877.,\n",
       "          49395., 62702., 57720., 64528., 12959., 51057., 44300., 44848., 85966.,\n",
       "          17839., 36108., 39763., 31085., 33514., 64844.,  7513., 83029., 25744.,\n",
       "          45962., 40195., 38969., 31801., 57898., 65318.,  1947., 27714., 65474.,\n",
       "          16807., 85393.,  5321.,  8064., 29208., 49895., 82958., 37301., 55790.,\n",
       "           6813., 50857., 28080.,  4651., 44732., 44132., 18209., 70211.,  5287.,\n",
       "           7896., 40195., 83869.,  6563., 82846., 44892., 79449., 65694., 47048.,\n",
       "          57989., 71211., 79424., 64541., 28739., 76667., 58889., 73393., 61362.,\n",
       "           3779., 28466., 23043.,  7896.,  5064., 71211., 58889., 88381., 86808.,\n",
       "          50230., 39634., 32751., 85963., 79667.,   757., 32519., 65742., 39296.,\n",
       "          27677.,  2557.],\n",
       "         [70240., 38153., 58889., 87897., 16921., 58142., 61945., 81654., 77744.,\n",
       "          58787., 80445., 59098., 78665.,  2104., 14113., 80445., 81605., 41201.,\n",
       "          30071., 56676., 35590.,  3556., 26430., 76791., 73686., 30518., 26337.,\n",
       "          61036., 14539., 17176., 78665., 29121.,  5064., 32240., 18502., 17740.,\n",
       "          88223., 32809., 23521., 73686., 40359., 18365., 61581., 44821., 57765.,\n",
       "          33543.,  2549., 11575., 38123., 52289., 71822., 75943., 35265., 10884.,\n",
       "          36595.,  1681., 23521., 23521., 46987.,  1693., 85966., 23337.,  9484.,\n",
       "          29851., 36595., 78665.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.]]),\n",
       " tensor([1, 0]),\n",
       " tensor([92, 66]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "### <your code> ###\n",
    "custom_dst = dataset(review_pairs, word2idx)\n",
    "custom_dataloader = DataLoader(custom_dst, collate_fn=collate_fn, shuffle=True, batch_size=2)\n",
    "next(iter(custom_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
