{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9, 5, 4, 4], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0586, 0.0987, 0.2047, 0.0624, 0.0742, 0.0750, 0.0976, 0.0858, 0.1048,\n",
      "         0.1382],\n",
      "        [0.0764, 0.1112, 0.0665, 0.0748, 0.0794, 0.1381, 0.2308, 0.0388, 0.0801,\n",
      "         0.1039],\n",
      "        [0.0816, 0.0807, 0.1878, 0.0731, 0.0591, 0.0736, 0.1021, 0.1279, 0.1566,\n",
      "         0.0574],\n",
      "        [0.0919, 0.0479, 0.1803, 0.0864, 0.0688, 0.0922, 0.1345, 0.1008, 0.1463,\n",
      "         0.0508]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3656, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0171, -0.0324,  0.0133,  ...,  0.0465, -0.0202,  0.0432],\n",
      "        [ 0.0230, -0.0523, -0.0493,  ..., -0.0475,  0.0181, -0.0511],\n",
      "        [ 0.0577, -0.0273, -0.0089,  ..., -0.0175, -0.0148, -0.0354],\n",
      "        ...,\n",
      "        [ 0.0578,  0.0419,  0.0256,  ..., -0.0077,  0.0530,  0.0195],\n",
      "        [ 0.0381, -0.0290, -0.0524,  ..., -0.0605, -0.0363, -0.0315],\n",
      "        [-0.0609,  0.0047,  0.0139,  ...,  0.0434, -0.0317, -0.0072]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 1.7681e-02,  3.0425e-02,  2.6314e-02,  ...,  2.4183e-03,\n",
      "          9.9028e-03, -1.4090e-02],\n",
      "        [ 2.0626e-02,  2.1725e-02, -2.4732e-02,  ..., -3.5873e-02,\n",
      "         -2.6912e-03, -5.1798e-02],\n",
      "        [ 5.1775e-08, -3.3999e-07,  4.1840e-07,  ..., -1.6427e-07,\n",
      "          2.1186e-07,  1.3377e-07],\n",
      "        ...,\n",
      "        [ 1.9677e-02,  2.0703e-02,  1.1523e-02,  ..., -7.4519e-03,\n",
      "          4.9417e-02, -2.2105e-02],\n",
      "        [-9.9166e-02, -8.6086e-02, -5.0260e-02,  ...,  8.8949e-02,\n",
      "         -1.0248e-01,  1.3786e-01],\n",
      "        [ 4.1233e-02,  6.4970e-02,  1.1566e-02,  ...,  2.2654e-03,\n",
      "          1.2694e-01, -4.8041e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0181, -0.0334,  0.0123,  ...,  0.0455, -0.0212,  0.0442],\n",
      "        [ 0.0220, -0.0533, -0.0483,  ..., -0.0465,  0.0171, -0.0501],\n",
      "        [ 0.0567, -0.0263, -0.0079,  ..., -0.0165, -0.0138, -0.0344],\n",
      "        ...,\n",
      "        [ 0.0568,  0.0409,  0.0246,  ..., -0.0067,  0.0520,  0.0205],\n",
      "        [ 0.0391, -0.0280, -0.0514,  ..., -0.0615, -0.0353, -0.0325],\n",
      "        [-0.0619,  0.0037,  0.0129,  ...,  0.0424, -0.0327, -0.0062]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 1.7681e-02,  3.0425e-02,  2.6314e-02,  ...,  2.4183e-03,\n",
      "          9.9028e-03, -1.4090e-02],\n",
      "        [ 2.0626e-02,  2.1725e-02, -2.4732e-02,  ..., -3.5873e-02,\n",
      "         -2.6912e-03, -5.1798e-02],\n",
      "        [ 5.1775e-08, -3.3999e-07,  4.1840e-07,  ..., -1.6427e-07,\n",
      "          2.1186e-07,  1.3377e-07],\n",
      "        ...,\n",
      "        [ 1.9677e-02,  2.0703e-02,  1.1523e-02,  ..., -7.4519e-03,\n",
      "          4.9417e-02, -2.2105e-02],\n",
      "        [-9.9166e-02, -8.6086e-02, -5.0260e-02,  ...,  8.8949e-02,\n",
      "         -1.0248e-01,  1.3786e-01],\n",
      "        [ 4.1233e-02,  6.4970e-02,  1.1566e-02,  ...,  2.2654e-03,\n",
      "          1.2694e-01, -4.8041e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0181, -0.0334,  0.0123,  ...,  0.0455, -0.0212,  0.0442],\n",
      "        [ 0.0220, -0.0533, -0.0483,  ..., -0.0465,  0.0171, -0.0501],\n",
      "        [ 0.0567, -0.0263, -0.0079,  ..., -0.0165, -0.0138, -0.0344],\n",
      "        ...,\n",
      "        [ 0.0568,  0.0409,  0.0246,  ..., -0.0067,  0.0520,  0.0205],\n",
      "        [ 0.0391, -0.0280, -0.0514,  ..., -0.0615, -0.0353, -0.0325],\n",
      "        [-0.0619,  0.0037,  0.0129,  ...,  0.0424, -0.0327, -0.0062]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
