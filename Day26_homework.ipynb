{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day26_範例_transformer_decoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gu1c8eWEQG76"},"source":["# 範例 : Transformer decoder\n","***\n","- 實做 Transformer decoder 以更了解　Transformer \n","- 應用 Transformer decoder 建立一個簡單的 ptt 貼文回應器 驗證 Transformer decoder 可以運行"]},{"cell_type":"markdown","metadata":{"id":"wZljCWHhScnI"},"source":["# [教學目標]\n","- 了解如何實作 transformer decoder 和其結構\n","- 了解如何應用 transformer decoder 並證明 decoder 可以作用\n"]},{"cell_type":"markdown","metadata":{"id":"C50wfDUxTFvZ"},"source":["# [範例重點]\n","- 觀察 TransformerDecoder 的建立\n","- 觀察 TransformerDecoderLayer 的建立\n","  - 使用 encoder 相同的 MultiHeadAttentionSubLayer\n","  - 使用 encoder 相同的 PosFeedForwardSubLayer\n","- 觀察如何使用 建立的 TransformerDecoder \n","  - 使用 TransformerDecoder 做序列生成 SequenceGenerate\n","  - 如何使用 SequenceGenerate 模型 訓練一個 ptt 回應機"]},{"cell_type":"markdown","metadata":{"id":"HNsXiF46TvpP"},"source":["# [範例結構]\n","- TransformerDecoder 模型和 SequenceGenerate 實作\n","- ptt 資料準備\n","- 應用 SequenceGenerate 訓練 ptt answer machine"]},{"cell_type":"code","metadata":{"id":"AOKbhccjXtg1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620891901147,"user_tz":-480,"elapsed":18920,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"6f9f5fda-f0af-4345-c527-416389a463bc"},"source":["# 連接個人資料 讀取 PTT 訓練資料和儲存模型\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAu2xwIt117c","executionInfo":{"status":"ok","timestamp":1620891905947,"user_tz":-480,"elapsed":743,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"9cc4ea5c-51cf-4f0c-83dc-bff9df6140a1"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Thu May 13 07:45:05 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GZm9DVn9-r4n","executionInfo":{"status":"ok","timestamp":1620894512458,"user_tz":-480,"elapsed":737,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["import re\n","import csv\n","import time\n","import torch\n","import torchtext\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import Counter\n","from torchtext.vocab import Vocab\n","from torchtext.data.utils import get_tokenizer\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGOuTtZbWhIj"},"source":["# 實做 TransformerDecoder\n","- 如果只用 Transfomer decoder 而已 不和 encoder　一起使用 \n","  - skip_encoder_attn 不需要和 encoder attention\n","  - enc_hidden　和 enc_mask　不用輸入"]},{"cell_type":"code","metadata":{"id":"SiWfhvskI2fH","executionInfo":{"status":"ok","timestamp":1620894514093,"user_tz":-480,"elapsed":700,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, hidden_dim, feedforward_dim, n_dec_layers, n_attn_heads, dropout, dec_voca_length, \n","               max_pos_length , device , skip_encoder_attn = False):\n","        # hidden_dim = 256\n","        # feedforward_dim = 512\n","        # n_dec_layers = 3 \n","        # n_attn_heads = 8 \n","        # dropout = 0.1\n","        # dec_voca_length = len(cmn_vocab)\n","        super().__init__()\n","        self.device = device\n","        # 建立 decoder token embedding \n","        self.dec_tok_embedding = nn.Embedding(dec_voca_length, hidden_dim)\n","        # 建立 decoder position embedding \n","        self.dec_pos_embedding = nn.Embedding(max_pos_length, hidden_dim)\n","\n","        # 建立 n_dec_layers 個 TransformerDecoderLayer 層\n","        self.transformer_decoder_layers = nn.ModuleList([TransformerDecoderLayer(hidden_dim,\n","                                                                                 feedforward_dim,\n","                                                                                 n_attn_heads, \n","                                                                                 dropout, \n","                                                                                 device, skip_encoder_attn) for _ in range(n_dec_layers)])\n","        # 輸出層 輸出 vocabulary 個長度\n","        self.full_conn_out = nn.Linear(hidden_dim, dec_voca_length)\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n","\n","    def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n","        # dec_seq shape: [batch size, dec seq len]\n","        # enc_hidden shape: [batch size, enc seq len, hid dim] # optional 不需要時輸入空值\n","        # dec_mask shape: [batch size, dec seq len]\n","        # enc_mask shape: [batch size, enc seq len] # optional 不需要時輸入空值\n","                    \n","        batch_size = dec_seq.shape[0]\n","        dec_len = dec_seq.shape[1]\n","            \n","        pos = torch.arange(0, dec_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)                      \n","        # pos shape: [batch size, dec seq len]\n","                \n","        # 將 decoder token embedding 加上 decoder postion embedding\n","        dec_seq = self.dropout(self.dec_tok_embedding(dec_seq)  + self.dec_pos_embedding(pos))\n","        # dec_seq shape: [batch size, dec seq len, hid dim]\n","            \n","        for layer in self.transformer_decoder_layers:\n","            dec_seq, encoder_decoder_attention , decoder_self_attention = layer(dec_seq, enc_hidden, dec_mask, enc_mask)\n","        # dec_seq shape: [batch size, dec seq  len, hid dim]\n","        # attention shape: [batch size, n heads, trg len, src len]\n","            \n","        output = self.full_conn_out(dec_seq)  \n","        # output shape: [batch size, trg len, output dim]\n","                \n","        return output, encoder_decoder_attention , decoder_self_attention"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNvw02zeMGIT"},"source":["# 實做 TransformerDecoderLayer\n","- 實作在transformerDecoder 使用多層 的TransformerDecoderLayer\n","- 如果只使用 decoder 則不用 encoder attention, --> skip_encoder_attn = True "]},{"cell_type":"code","metadata":{"id":"gAV9O7SnJsLU","executionInfo":{"status":"ok","timestamp":1620894514428,"user_tz":-480,"elapsed":640,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(self, hidden_dim , feedforward_dim, n_attn_heads, dropout , device , skip_encoder_attn = False):\n","        # hidden_dim = 256\n","        # feedforward_dim = 512\n","        # n_attn_heads = 8\n","        # dropout = 0.1\n","        super().__init__()\n","        self.skip_encoder_attn = skip_encoder_attn \n","        self.self_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n","        self.self_attn_layernorm = nn.LayerNorm(hidden_dim)\n","\n","        if not skip_encoder_attn:\n","            self.encoder_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n","            self.encoder_attn_layernorm = nn.LayerNorm(hidden_dim)\n","\n","        self.positionwise_feedforward = PosFeedForwardSubLayer(hidden_dim, feedforward_dim, dropout)\n","        self.feedforward_layernorm = nn.LayerNorm(hidden_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)     \n","\n","    def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n","        # dec_seq    shape: [batch size, dec seq len, hid dim]\n","        # enc_hidden shape: [batch size, enc seq len, hid dim] # optional 不需要時輸入空值\n","        # dec_mask   shape: [batch size, dec seq len]\n","        # enc_mask   shape: [batch size, enc seq len] # optional 不需要時輸入空值\n","        \n","        # self attention 子層\n","        _dec_seq, decoder_self_attention = self.self_attention_sublayer(dec_seq, dec_seq, dec_seq, dec_mask)\n","            \n","        # dropout, residual connection and layer norm　(Add and Norm)\n","        dec_seq = self.self_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n","        # dec_seq  shape: [batch size, decode sequence len, hid dim]\n","                \n","        # 需不需要建立　encoder attention 層        \n","        if not self.skip_encoder_attn:\n","            # encoder attention\n","            _dec_seq, encoder_decoder_attention = self.encoder_attention_sublayer(dec_seq, enc_hidden, enc_hidden, enc_mask)\n","            # dropout, residual connection and layer norm\n","            dec_seq = self.encoder_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n","        else:\n","            encoder_decoder_attention = None\n","                        \n","        # dec_seq shape: [batch size, decode sequence len, hid dim]\n","        # positionwise feedforward\n","        _dec_seq = self.positionwise_feedforward(dec_seq)\n","            \n","        # dropout, residual and layer norm (Add and Norm)\n","        dec_seq = self.feedforward_layernorm(dec_seq + self.dropout(_dec_seq))\n","        # dec_seq shape: [batch size, decode sequence len, hid dim]\n","        # attention shape: [batch size, n heads, decode sequence len, encode sequence len]\n","            \n","        return dec_seq, encoder_decoder_attention , decoder_self_attention"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MZZrOaLOEQn"},"source":["# 實做 MultiHeadAttentionSubLayer\n","- 實作 encoder and decoder 同時共用的 MultiHeadAttention SubLayer \n"]},{"cell_type":"code","metadata":{"id":"j_9DTM9jJyDR","executionInfo":{"status":"ok","timestamp":1620894514805,"user_tz":-480,"elapsed":582,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["class MultiHeadAttentionSubLayer(nn.Module):\n","    def __init__(self, hidden_dim , n_attn_heads, dropout, device):\n","        # hidden_dim = 256\n","        # n_attn_heads = 8\n","        # dropout = 0.1\n","        super().__init__()\n","        # 確定設定的 hidden layer 維度可以被 attention head 整除\n","        assert hidden_dim % n_attn_heads ==0\n","\n","        # hidden layer 維度\n","        self.hidden_dim = hidden_dim\n","        # num of heads\n","        self.n_attn_heads = n_attn_heads\n","        # 平均分到每個 multi-head 的 維度\n","        self.head_dim = hidden_dim // n_attn_heads\n","        # 就是在課程中提到的 Wq Wk Wv\n","        self.full_conn_q = nn.Linear(hidden_dim, hidden_dim)\n","        self.full_conn_k = nn.Linear(hidden_dim, hidden_dim)\n","        self.full_conn_v = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # 最後結果再過一層 線性轉換\n","        self.full_conn_o = nn.Linear(hidden_dim, hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        # 根據維度大小調整 attention 值 以免維度太大 Q dot K 結果過大影響學習效率    \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","\n","    def forward(self, query_input, key_input, value_input, mask = None):\n","        batch_size = query_input.shape[0]\n","        # query_input shape [batch size, query len, hid dim]\n","        # key_input   shape [batch size, key len, hid dim]\n","        # value_input shape [batch size, value len, hid dim]\n","\n","        Q = self.full_conn_q(query_input)\n","        K = self.full_conn_k(key_input)\n","        V = self.full_conn_v(value_input)\n","        # Q shape [batch size, query len, hid dim]\n","        # K shape [batch size, key len, hid dim]\n","        # V shape [batch size, value len, hid dim]\n","\n","        # 將 attention 切成多塊小的 attention 並將 attention 的 2 和 3 維度轉置 以達到將 attention head 提到前面而分開每個 attention head\n","        Q = Q.view(batch_size, self.n_attn_heads, -1, self.head_dim)\n","        K = K.view(batch_size, self.n_attn_heads, -1, self.head_dim)\n","        V = V.view(batch_size, self.n_attn_heads, -1, self.head_dim)\n","        \n","        # 調整過的 dot product attention, 由於之前分開了每個 attention head \n","        # 所以現在只要把 Ｋ的最後兩個維度轉置 就可以 by attention head 求得 Q dot K\n","        scaled_dot_product_similarity = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        # scaled_dot_product_similarity 輸出 [batch size, n heads, query len, key len]\n","\n","        if mask is not None:\n","            scaled_dot_product_similarity = scaled_dot_product_similarity.masked_fill(mask == 0, -1e10)\n","\n","        attention = torch.softmax(scaled_dot_product_similarity, dim = -1)\n","        # attention shape: [batch size, n heads, query len, key len]\n","\n","        x = torch.matmul(self.dropout(attention), V)  \n","        # x shape: [batch size, n heads, query len, head dim]\n","            \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        # x shape: [batch size, query len, n heads, head dim]\n","            \n","        x = x.view(batch_size, -1, self.hidden_dim)  \n","        # x shape: [batch size, query len, hid dim]\n","            \n","        x = self.full_conn_o(x)  \n","        # x shape: [batch size, query len, hid dim]\n","            \n","        return x, attention"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4kg56sgOzpj"},"source":["# 實做 PosFeedForwardSubLayer\n","- 實作 encoder and decoder 同時共用的 PosFeedForward SubLayer "]},{"cell_type":"code","metadata":{"id":"Vv_5iQiFJ37I","executionInfo":{"status":"ok","timestamp":1620894515075,"user_tz":-480,"elapsed":518,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["class PosFeedForwardSubLayer(nn.Module):\n","    def __init__(self, hidden_dim, ff_dim, dropout):\n","      super().__init__()\n","      self.full_conn_1 = nn.Linear(hidden_dim, ff_dim)\n","      self.full_conn_2 = nn.Linear(ff_dim,  hidden_dim)\n","      self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","      # x shape: [batch size, seq len, hid dim]   \n","      x = self.dropout(torch.relu(self.full_conn_1(x)))  \n","      # x shape: [batch size, seq len, pf dim]\n","          \n","      x = self.full_conn_2(x)\n","      # x shape: [batch size, seq len, hid dim]\n","          \n","      return x"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Awfn3ELO8sg"},"source":["# 實做 SequenceGenerate \n","- 處理 序列生成工作\n","- 叫用 TransformerDecoderLayer\n","  - 不使用 encoder decoder attention 子層\n"]},{"cell_type":"code","metadata":{"id":"URWQcxLJJ63p","executionInfo":{"status":"ok","timestamp":1620894516069,"user_tz":-480,"elapsed":1143,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["class SequenceGenerate(nn.Module):\n","    def __init__(self, decoder, dec_pad_idx, device):\n","        super().__init__()\n","        self.decoder = decoder\n","        self.dec_pad_idx = dec_pad_idx\n","        self.device = device\n","\n","    def make_dec_mask(self, dec_seq):    \n","      # dec_seq shape: [batch size, dec seq len]   \n","      dec_pad_mask = (dec_seq != self.dec_pad_idx).unsqueeze(1).unsqueeze(2)  \n","      # dec_pad_mask shape: [batch size, 1, 1, dec seq len]\n","      \n","      dec_len = dec_seq.shape[1]  \n","      dec_sub_mask = torch.tril(torch.ones((dec_len, dec_len), device = self.device)).bool()\n","      # dec_sub_mask shape: [dec seq len, dec seq len]\n","              \n","      dec_mask = dec_pad_mask & dec_sub_mask  \n","      # dec_mask shape: [batch size, 1, dec seq len, dec seq len]\n","\n","      return dec_mask\n","\n","    def forward(self, dec_seq):\n","      # dec_seq shape:　tensor [batch size, trg len]          \n","      dec_mask = self.make_dec_mask(dec_seq)\n","      # dec_mask shape: [batch size, 1, trg len, trg len]\n","          \n","      # 呼叫　transformer decoder 不需要輸入　encoder 相關資訊\n","      # 也不用接收　encoder decoder attnetion            \n","      output, _ , decoder_self_attention = self.decoder(dec_seq, None, dec_mask, None)\n","      # output shape: [batch size, trg len, output dim]\n","      # attention shape: [batch size, n heads, trg len, src len]\n","          \n","      return output, decoder_self_attention"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wM76XyjFSw-m"},"source":["# PTT 資料準備\n","\n","- 我們的資料來源是 https://github.com/zake7749/Gossiping-Chinese-Corpus\n","- 詳情請看 github\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRBsuTjcSxhm","executionInfo":{"status":"ok","timestamp":1620894519064,"user_tz":-480,"elapsed":2863,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"c69325f2-1d72-45d7-e283-3126f0a16dd8"},"source":["data_dir = '/content/drive/My Drive/DL_NLP_marathon/data/Day26_transformer_decoder/'\n","with open(data_dir + 'Gossiping-QA-Dataset-2_0.csv' , encoding='utf-8') as fin:\n","    csvreader = csv.reader(fin)\n","    next(csvreader)\n","    ptt_qa_pairs = [row for row in csvreader]\n","\n","print (\"Sample: \" , ptt_qa_pairs[1000])\n","print (\"Total records:\" , len(ptt_qa_pairs))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Sample:  ['有沒有跑車很常見高級轎車卻很少的八卦', '高雄常看到賓利啊…勞斯萊斯就真的只看過兩次']\n","Total records: 774114\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"POdVnkF5XZRg"},"source":["# do training test split 如果已經分過了 可以跳過這段"]},{"cell_type":"code","metadata":{"id":"eDH9OthiSzqr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620878803246,"user_tz":-480,"elapsed":7327,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"aff7dbaa-7190-4478-869e-271f13dc967e"},"source":["from sklearn.model_selection import train_test_split\n","\n","train, val = train_test_split(ptt_qa_pairs, test_size=70000)\n","print (\"training data:{} , val data: {} \".format(len(train),len(val)))\n","    \n","def write_csv(trn_data, file_path):\n","    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n","        writer = csv.writer (fout)\n","        for itm in trn_data: \n","            writer.writerow ([itm[0] + \"|\" + itm[1] , itm[0] + \"|\" + itm[1]])\n","            \n","file_path = data_dir + 'train.csv'\n","write_csv(train, file_path)\n","\n","file_path = data_dir + 'val.csv'\n","write_csv(val, file_path)\n","\n","# file_path = data_dir + 'test.csv'\n","# write_csv(test, file_path )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training data:704114 , val data: 70000 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5lVcYLXjTDf4"},"source":["# 資料處理"]},{"cell_type":"code","metadata":{"id":"MtOsmKwC95yk","executionInfo":{"status":"ok","timestamp":1620894532710,"user_tz":-480,"elapsed":11371,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def qa_tokenizer(text):\n","  # 去掉非中文字元\n","  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n","  text = text.replace(\"\\\\\",\"\").split(\"|\")\n","  return [word for word in regex.sub(text[0],' ') if word.strip()] + [\"<sep>\"] + [word for word in regex.sub(text[1],' ') if word.strip()]\n","\n","def trg_tokenizer(text):\n","  # 去掉非中文字元\n","  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n","  text = text.replace(\"\\\\\",\"\").split(\"|\")\n","  return ['<pad>' for word in regex.sub(text[0],' ') if word.strip()] + [\"<pad>\"] + [word for word in regex.sub(text[1],' ') if word.strip()] \n","\n","def build_vocab(filepath, tokenizer):\n","    counter = Counter()\n","    with open(filepath, encoding=\"utf8\") as f:\n","        for string_ in f:\n","            counter.update(tokenizer(string_))\n","    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>', '<sep>'])\n","\n","\n","train_filepath = data_dir + 'train.csv'\n","val_filepath = data_dir + 'val.csv'\n","cmn_vocab = build_vocab(train_filepath, qa_tokenizer)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mcl_m4k1_nwG","executionInfo":{"status":"ok","timestamp":1620894573278,"user_tz":-480,"elapsed":51705,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def data_process(filepath):\n","    data = []\n","    with open(filepath, encoding='utf-8') as fin:\n","        csvreader = csv.reader(fin)\n","        for i, row in enumerate(csvreader):\n","            qa, trg = row[0], row[1]\n","            qa_tensor_ = torch.tensor([cmn_vocab[token] for token in qa_tokenizer(qa)], \n","                                    dtype=torch.long)\n","            trg_tensor_ = torch.tensor([cmn_vocab[token] for token in trg_tokenizer(trg)],\n","                                    dtype=torch.long)\n","            data.append((qa_tensor_, trg_tensor_))\n","    return data\n","\n","train_data = data_process(train_filepath)\n","val_data = data_process(val_filepath)\n","# test_data = data_process(test_filepaths)"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rMbQIsMV0pF"},"source":["# 我們要使用的資料格式\n","- 建立 vocabulary\n","- qa: ptt 上蒐集的問題和回答中間用 “sep”隔開\n","- trg: 我們的訓練目標只有回答的部分，其他的字元（包括“\\<sep\\>”）我們都以 “\\<pad\\>” 取代 , 計算 loss 的時候系統會忽略 ”pad“ token 註記的目標"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5osxZGhOiUD","executionInfo":{"status":"ok","timestamp":1620894574258,"user_tz":-480,"elapsed":945,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"ba6be21d-4d78-41a5-affe-37be592a4aef"},"source":["print (\"中文語料的字元表長度: \" , len(cmn_vocab))\n","print (\"Sample Q and A:\", [cmn_vocab.itos[idx] for idx in val_data[2000][0]])\n","print (\"Sample Target:\",  [cmn_vocab.itos[idx] for idx in val_data[2000][1]])"],"execution_count":52,"outputs":[{"output_type":"stream","text":["中文語料的字元表長度:  7645\n","Sample Q and A: ['約', '克', '夏', '與', '約', '克', '羊', '哪', '個', '好', '養', '<sep>', '牠', '最', '近', '沒', '粗', '乃', '喔']\n","Sample Target: ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '牠', '最', '近', '沒', '粗', '乃', '喔']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WydwZItmV2KF"},"source":["# 準備 train_iterator and valid_iterator"]},{"cell_type":"code","metadata":{"id":"AUremXoSk5XC","executionInfo":{"status":"ok","timestamp":1620894574259,"user_tz":-480,"elapsed":924,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["BATCH_SIZE = 768\n","\n","PAD_IDX = cmn_vocab['<pad>']\n","BOS_IDX = cmn_vocab['<bos>']\n","EOS_IDX = cmn_vocab['<eos>']\n","\n","def generate_batch(data_batch):\n","    qa_batch, trg_batch = [], []\n","    for (qa_item, trg_item) in data_batch:\n","        qa_batch.append(torch.cat([torch.tensor([BOS_IDX]), qa_item, torch.tensor([EOS_IDX])], dim=0))\n","        trg_batch.append(torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0))\n","    \n","    sorted_idx = sorted(range(len(qa_batch)), key=lambda i: len(qa_batch[i]), reverse=True)\n","    qa_batch = [qa_batch[i] for i in sorted_idx]\n","    trg_batch = [trg_batch[i] for i in sorted_idx]\n","    qa_batch = pad_sequence(qa_batch, padding_value=PAD_IDX, batch_first=True)\n","    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n","    return qa_batch, trg_batch\n","\n","\n","train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n","                        shuffle=True, collate_fn=generate_batch)\n","valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n","                        shuffle=True, collate_fn=generate_batch)\n","# test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n","#                        shuffle=True, collate_fn=generate_batch)"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFTfx3TuR-5Z"},"source":["# model training and evaluate function\n","- 注意 我們要輸入的字和目標要shift 一位 \n","- 也就是輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>' --> 希望輸出 '有'\n","- 輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>', '有' --> 希望輸出 '的'"]},{"cell_type":"code","metadata":{"id":"LIcei9Ooblb9","executionInfo":{"status":"ok","timestamp":1620894574668,"user_tz":-480,"elapsed":1305,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def train(model, iterator, optimizer, criterion, clip):    \n","    model.train()\n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        qa = batch[0].to(device)\n","        trg = batch[1].to(device)\n","        # qa shape: [batch size, qa len]\n","        # trg shape: [batch size, trg len]\n","        \n","        optimizer.zero_grad()\n","        output, _  = model(qa[:, :-1])\n","        # output shape: [batch size, trg len - 1, output dim]\n","\n","        output_dim = output.shape[-1]\n","        \n","        output = output.contiguous().view(-1, output_dim)\n","        trg = trg[:, 1:].contiguous().view(-1)\n","        # output shape: [batch size * (trg len - 1), output dim]\n","        # trg    shape: [batch size * (trg len - 1)]\n","        \n","        loss = criterion(output, trg)\n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","        if i % 1000 == 0: print (\"Train Batch:\" , i , \"Loss:\" , loss.item())\n","\n","    return epoch_loss / len(iterator)"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"AobBpcaSboe8","executionInfo":{"status":"ok","timestamp":1620894574669,"user_tz":-480,"elapsed":1291,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def evaluate(model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            qa = batch[0].to(device)\n","            trg = batch[1].to(device)\n","            # qa shape: [batch size, qa len]\n","            # trg shape: [batch size, trg len]\n","            \n","            output, _  = model(qa[:, :-1])\n","            # output shape: [batch size, (trg len - 1), output dim]\n","            \n","            output_dim = output.shape[-1]\n","            \n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:, 1:].contiguous().view(-1)\n","            # output shape: [batch size * (trg len - 1), output dim]\n","            # trg    shape: [batch size * (trg len - 1)]\n","            \n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asQT783VaaUU"},"source":["# 實際建立模型\n","- 設定重要參數\n","  - 建立一個 hidden embedding 256，三層decoder layer，八個attention heads\n","  - position wise feedforward 中間層 512 dropout 0.1 learning rate: 0.0005\n","  - 最長句長 70\n","- 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"]},{"cell_type":"code","metadata":{"id":"QtWL65_Ha1dX","executionInfo":{"status":"ok","timestamp":1620894574669,"user_tz":-480,"elapsed":1283,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["model_dir = '/content/drive/My Drive/DL_NLP_marathon/model/Day26_transformer/'\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","VOC_SIZE = len(cmn_vocab)\n","MAX_SENT_LENGTH = 70\n","HID_DIM = 256\n","DEC_LAYERS = 3\n","DEC_HEADS = 8\n","DEC_FF_DIM = 512\n","DEC_DROPOUT = 0.1\n","LEARNING_RATE = 0.0005\n","CMN_PAD_IDX = cmn_vocab['<pad>']\n","\n","dec = TransformerDecoder(HID_DIM, \n","                         DEC_FF_DIM,\n","                         DEC_LAYERS, \n","                         DEC_HEADS,  \n","                         DEC_DROPOUT,\n","                         VOC_SIZE, \n","                         MAX_SENT_LENGTH,\n","                         device, skip_encoder_attn=True)\n","\n","# TransformerSequenceGenerate\n","model = SequenceGenerate(dec, CMN_PAD_IDX, device).to(device)\n","optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n","criterion = nn.CrossEntropyLoss(ignore_index = CMN_PAD_IDX)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDXajaXalpKX","executionInfo":{"status":"ok","timestamp":1620894574670,"user_tz":-480,"elapsed":1276,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)\n","model.apply(initialize_weights);"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOnClWtxc8VI"},"source":["# 實際訓練\n","- Ｔ4 大約 四分半一個 epoch\n","- 訓練十個 epoch 就有一定的成績了\n","- 如果沒時間訓練 也可以下載我們訓練好的權重"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"7x3EMi_px0aw","executionInfo":{"status":"error","timestamp":1620896127681,"user_tz":-480,"elapsed":1554276,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"eebc1aea-9fc7-421d-bf60-0f5abbe10249"},"source":["N_EPOCHS = 100\n","CLIP = 1\n","\n","best_valid_loss = 9999999\n","\n","for epoch in range(N_EPOCHS):    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","    \n","    # epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    torch.save(model.state_dict(), model_dir + 'model-ptt-{}.pt'.format(epoch))\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), model_dir + 'model-ptt-best.pt')\n","\n","    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format(epoch, \n","                                                                                                      end_time - start_time, \n","                                                                                                      train_loss , \n","                                                                                                      valid_loss))"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Train Batch: 0 Loss: 8.970396995544434\n","Epoch 0 training time: 370.28 sec Training Loss: 4.121 , Valiation Loss: 1.920\n","Train Batch: 0 Loss: 1.7315258979797363\n","Epoch 1 training time: 368.00 sec Training Loss: 1.442 , Valiation Loss: 1.098\n","Train Batch: 0 Loss: 1.1887110471725464\n","Epoch 2 training time: 368.64 sec Training Loss: 1.034 , Valiation Loss: 0.633\n","Train Batch: 0 Loss: 0.790983259677887\n","Epoch 3 training time: 372.20 sec Training Loss: 0.580 , Valiation Loss: 0.330\n","Train Batch: 0 Loss: 0.30305853486061096\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-10277e11a3f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-d0ec3a023f73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfTyGsYK-Tu2","executionInfo":{"status":"ok","timestamp":1620896132485,"user_tz":-480,"elapsed":820,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"dea7ca97-c70d-415b-d1fb-93ce7721b831"},"source":["!nvidia-smi"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Thu May 13 08:55:32 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   74C    P0    34W /  70W |  14656MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ePu_d6fSbLkZ"},"source":["# 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"]},{"cell_type":"code","metadata":{"id":"jGtaydsuHQ2X","executionInfo":{"status":"ok","timestamp":1620896137832,"user_tz":-480,"elapsed":737,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["model_dir = '/content/drive/My Drive/DL_NLP_marathon/model/Day26_transformer/'\n","torch.save(cmn_vocab, model_dir + 'vocab.pt')"],"execution_count":61,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZC5fZzIUeByb"},"source":["# 讀取訓練最佳結果\n","-- 如果下載我們的訓練結果 別忘了讀取 vocabulary"]},{"cell_type":"code","metadata":{"id":"HLq4YhL1bFa4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620896156420,"user_tz":-480,"elapsed":14637,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"e9d479a5-a6f6-4307-98a8-67ff25b798a1"},"source":["# 保留讀取之前儲存的 vocabulary\n","cmn_vocab = torch.load(model_dir + 'vocab.pt')\n","\n","model.load_state_dict(torch.load(model_dir + 'model-ptt-best.pt'))\n","# model.load_state_dict(torch.load(model_dir + 'model-ptt-1.pt'))\n","test_loss = evaluate(model, valid_iter, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f}')"],"execution_count":62,"outputs":[{"output_type":"stream","text":["| Test Loss: 0.250\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dsGJujTfe3b5"},"source":["# 使用訓練結果產生回答\n","- 用模型每一步最佳猜測產生回答"]},{"cell_type":"code","metadata":{"id":"Sguk0pbPPT20","executionInfo":{"status":"ok","timestamp":1620896161162,"user_tz":-480,"elapsed":668,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}}},"source":["def simple_answer_ptt_question(sentence, qa_vocab, model, device, max_len = 50):   \n","    model.eval()\n","        \n","    tokens = [token.lower() for token in sentence]\n","    tokens = ['<bos>'] + tokens + ['<sep>']\n","    qa_indexes = [qa_vocab[token] for token in tokens]\n","    # qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n","    \n","    for i in range(max_len):\n","        qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            dec_qa, decoder_self_attention = model(qa_tensor)\n","        \n","        pred_token = dec_qa.argmax(2)[:,-1].item()\n","        qa_indexes.append(pred_token)\n","\n","        if pred_token == qa_vocab[\"<eos>\"]:\n","            break\n","    \n","    # answer = \"\".join([qa_vocab.itos[i] for i in qa_indexes])\n","    # print(qa_indexes)\n","    # qa_tokens = [qa_vocab.itos[i] for i in qa_indexes]\n","    # answer = \"\".join(qa_tokens)\n","    qa_tokens = [qa_vocab.itos[i] for i in qa_indexes]\n","    answer = \"\".join(qa_tokens)\n","    # answer = \"\".join(qa_tokens[qa_tokens.index(\"<sep>\")+1:-1])\n","            \n","    return answer,  decoder_self_attention"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBrYey4fetvJ"},"source":["# Fun Time\n","-- 自己上 ptt 找新的標題來玩吧"]},{"cell_type":"code","metadata":{"id":"pUFx32UDKE9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620896186817,"user_tz":-480,"elapsed":692,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"c76baf42-f19c-45e3-d65a-72a2e241349a"},"source":["question = \"日月光是找老婆的好地方嗎\"\n","# question = '長這麼大，做過最壞的事是什麼？'\n","# question = '看到前女友生小孩是什麼感覺'\n","# question = '把中國人惹翻了會怎麼樣嗎？'\n","# question = '泰國人民為何不推翻王室?'\n","qa_result, _ = simple_answer_ptt_question(question, cmn_vocab, model, device, max_len = 50)\n","\n","print(qa_result)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["<bos>日月光是找老婆的好地方嗎<sep>高雄，嗎.高高興趣的比較多的比較有多的<eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDbN_pwl08Ou","executionInfo":{"status":"ok","timestamp":1620894299703,"user_tz":-480,"elapsed":762,"user":{"displayName":"chi-fen Liao","photoUrl":"","userId":"16137842954465597187"}},"outputId":"16ab4ddf-1921-4aed-944d-d24c86dbbb79"},"source":["_.size()"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 8, 17, 17])"]},"metadata":{"tags":[]},"execution_count":40}]}]}