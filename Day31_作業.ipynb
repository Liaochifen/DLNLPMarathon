{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWmYuIqwXky9"
   },
   "source": [
    "# 作業 : 自行調整完整版 Bert 預訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjR0X3RKXkzH"
   },
   "source": [
    "# [作業目標]\n",
    "- 觀察並了解調整 Step4.3 類神經網路結構對結果帶來的影響\n",
    "- 觀察並了解調整 Step4.2 Batch Size 以及 Step4.4 的Optimizer & Learning Rate 對結果帶來的影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9OcW6i0XkzI"
   },
   "source": [
    "# [作業重點]\n",
    "- 程式最後會輸出 Kaggle 練習題的提交檔, 同學可以藉由提交分數驗證結果\n",
    "- 請同學在修改時, 記得以檔名或其他形式保留調整的紀錄, 以免忘記最佳輸出的調整方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1622120759857,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "OCDU1lzehKJE",
    "outputId": "263b4b40-1440-4835-c4e8-6b3efe6a99b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1622120768813,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "Z11gzvYoxjJ3",
    "outputId": "1a40d965-dcd3-46ba-a54d-c3f0f11b411f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 27 13:06:08 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   56C    P0    28W /  70W |  10028MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HiLyC9ZXkzJ"
   },
   "source": [
    "# 載入資料與套件, 進行切割與預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1622120772442,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "eVDiQwbCXkzJ"
   },
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1622120773995,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "UngfxH5AXkzK"
   },
   "outputs": [],
   "source": [
    "# 將訓練資料切割成 訓練集 / 驗證集\n",
    "filepath = 'data/'\n",
    "df =  pd.read_csv(filepath + 'train.csv')\n",
    "X = df.text.values\n",
    "y = df.target.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1622120775216,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "v6PqS3WeXkzK",
    "outputId": "13245455-c145-42ae-efce-2205bde4152a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>749</td>\n",
       "      <td>Car Receiving tube Installation for Avalanche ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>8474</td>\n",
       "      <td>@frailnerves I SCREAMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>4566</td>\n",
       "      <td>Emergency-response plan helps employees get ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>5158</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>7469</td>\n",
       "      <td>????\\nWhy did God order obliteration of ancien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "230    749  Car Receiving tube Installation for Avalanche ...\n",
       "2540  8474                            @frailnerves I SCREAMED\n",
       "1385  4566  Emergency-response plan helps employees get ba...\n",
       "1544  5158  11-Year-Old Boy Charged With Manslaughter of T...\n",
       "2243  7469  ????\\nWhy did God order obliteration of ancien..."
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 載入測試資料\n",
    "test_df = pd.read_csv(filepath + 'test.csv')\n",
    "test_df = test_df[['id', 'text']]\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622120776645,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "e_wESyxZXkzM",
    "outputId": "de97d335-bcf8-4644-e440-bddab0f1259f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# 載入 pytorch 套件, 依照現有環境判定是否使用 GPU 計算\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622120777290,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "4P4Azey2XkzN"
   },
   "outputs": [],
   "source": [
    "# 簡化版前處理\n",
    "def text_preprocessing(text):\n",
    "    # 移除推特的姓名標籤 ('@name')\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    # 將 '&amp;' 替換成 '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    # 移除文末的空白字元\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1622120779669,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "RdhdTZhXXkzN",
    "outputId": "f2f3fa84-c5db-4f15-98d3-a58c1e0635a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  mom: 'we didn't get home as fast as we wished' \n",
      "me: 'why is that?'\n",
      "mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n",
      "Processed:  mom: 'we didn't get home as fast as we wished' me: 'why is that?' mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n"
     ]
    }
   ],
   "source": [
    "# 印出第一組推文在前處理之前與之後的改變\n",
    "idx = 80\n",
    "print('Original: ', X[idx])\n",
    "print('Processed: ', text_preprocessing(X[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZYjqFAZXkzN"
   },
   "source": [
    "# Step 4.1 : 載入 Bert 套件與 tokenizer, 將本文編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "executionInfo": {
     "elapsed": 5990,
     "status": "ok",
     "timestamp": 1622118321780,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "FMA12B3vkcTD",
    "outputId": "09733d7b-c154-4c9d-f5c4-9a9f47894d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
      "\r",
      "\u001b[K     |▏                               | 10kB 24.4MB/s eta 0:00:01\r",
      "\u001b[K     |▎                               | 20kB 30.1MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 30kB 35.7MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 40kB 26.7MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 51kB 28.2MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 61kB 30.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 71kB 29.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 81kB 27.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 92kB 29.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 102kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 112kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 122kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 133kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 143kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 153kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 163kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 174kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 184kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 194kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 204kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 215kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 225kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 235kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 245kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 256kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 266kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 276kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 286kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 296kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 307kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 317kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 327kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 337kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 348kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 358kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 368kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 378kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 389kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 399kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 409kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 419kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 430kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 440kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 450kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 460kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 471kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 481kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 491kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 501kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 512kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 522kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 532kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 542kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 552kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 563kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 573kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 583kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 593kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 604kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 614kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 624kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 634kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 645kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 655kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 665kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 675kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 686kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 696kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 706kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 716kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 727kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 737kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 747kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 757kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 768kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 778kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 788kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 798kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 808kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 819kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 829kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 839kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 849kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 860kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 870kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 880kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 890kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 901kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 911kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 921kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 931kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 942kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 952kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 962kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 972kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 983kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 993kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 1.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 1.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 1.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 1.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 1.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 1.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 1.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 1.3MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 1.4MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.5MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 1.6MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 1.7MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.8MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.9MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 2.0MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 2.1MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 2.2MB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 2.3MB 27.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 50.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 40.9MB/s \n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4.6.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1622120784413,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "dnA2rPKYXkzO"
   },
   "outputs": [],
   "source": [
    "# 載入 Bert 套件與 tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# 設定 Bert 的前處理函數\n",
    "def preprocessing_for_bert(data):\n",
    "    # 初始化要傳回的資料\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # 把所有文句用 tokenizer 編碼\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer(\n",
    "            text=text_preprocessing(sent),  # 套用簡化版前處理函數\n",
    "            add_special_tokens=True,        # 加上 `[CLS]` 與 `[SEP]`\n",
    "            padding='max_length',           # 填充至指定長度\n",
    "            pad_to_max_length=True,         # 是否要填充到最大長度\n",
    "            return_attention_mask=True      # 是否傳回 attention mask\n",
    "        )\n",
    "        # 更新要傳回的資料\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    # 將傳回資料轉為 tensor\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6179,
     "status": "ok",
     "timestamp": 1622118332934,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "D9ezEZMyXkzO",
    "outputId": "1b70dfb1-a100-452d-847e-7e70dcdbce33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  84\n"
     ]
    }
   ],
   "source": [
    "# 將訓練資料與測試資料的\"推文\"合併\n",
    "all_tweets = np.concatenate([df.text.values, test_df.text.values])\n",
    "\n",
    "# 將推文使用 tokenizer 加以編碼\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "\n",
    "# 找出最大的推文長度 (訓練資料 + 預測目標資料)\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5486,
     "status": "ok",
     "timestamp": 1622120793731,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "ov-IWDOKXkzP",
    "outputId": "aa551cf1-89c9-4954-972d-a06554818783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Token IDs:  [101, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# 將上一格的 Max length 數值填入\n",
    "max_length = 84\n",
    "\n",
    "# 顯示第一筆資料的推文與期經過 Bert 的前處理函數 (preprocessing_for_bert) 的編碼結果 (確認函數正確)\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# 使用 preprocessing_for_bert 將訓練 / 驗證集的推文進行編碼\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY6IgL0KXkzP"
   },
   "source": [
    "# Step 4.2 : Fine Tune 前的準備 - 設定 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1622120793731,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "2iDpz5UUXkzQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 將訓練與驗證目標值轉為 torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# 要微調 (fine-tuning) BERT 時, 原作者建議的 batch size 為 16 或 32\n",
    "batch_size = 8\n",
    "\n",
    "# 設定訓練與驗證集的 DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFNtfKhaXkzQ"
   },
   "source": [
    "# Step 4.3 : 設定 Bert 連接目標值的 Layer 結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1622120795855,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "gyE5aOSjXkzR",
    "outputId": "6f074284-1c7c-4a60-9be9-55b189ce6e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42 µs, sys: 0 ns, total: 42 µs\n",
      "Wall time: 45.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 載入 pytorch 與 Bert 相關套件\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# 自定義 Bert 分類器函數\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # 指定 BERT 輸入長度大小(D_in), 分類器的隱藏層大小(H), 以及分類目標值的種類數量(D_out)\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "        # 載入 Bert 預訓練權重作為初始值\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # 初始化自定義分類器的類神經網路\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        # 凍結 Bert 部分的權重\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 將資料輸入 BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)        \n",
    "        # 將輸出結果存在 last_hidden_state_cls 中\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # 將輸出結果輸入自定義分類器\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOxS-dcOXkzS"
   },
   "source": [
    "# Step 4.4 : Optimizer & Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1622120798335,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "g6ihaa8zXkzS"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    # 初始化 Bert 分類器\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    # 告訴 PyTorch 模型需要在 GPU 上執行\n",
    "    bert_classifier.to(device)\n",
    "    # 設定 optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # 預設的學習速率\n",
    "                      eps=1e-8    # 預設的 epsilon 值\n",
    "                      )\n",
    "    # 計算總共的訓練步數\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # 設定學習率排程\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # 預設值\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1622120802149,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "5YvwYTdLXkzT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# 設定損失函數\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    # 開始訓練迴圈\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # 印出變數表格標題\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "        # 測量每個 epoch 的執行時間\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        # 每個 epoch 開始時重置追蹤的變數\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        # 將模型切換到訓練模式\n",
    "        model.train()\n",
    "        # 訓練資料的每個 batch \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # 將模型中之前計算的梯度歸零\n",
    "            model.zero_grad()\n",
    "\n",
    "            # 將所有 batch 資料載入 GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            # 執行一個向前傳遞. 這會傳回一個 logit 值\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # 計算並累加損失值\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 執行一個向後傳遞以計算梯度\n",
    "            loss.backward()\n",
    "            # 將梯度侷限在正負 1 範圍內, 防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 更新參數與學習率\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # 每 20 個 batches 印出損失值與執行時間一次\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # 計算 20 batches 的執行時間\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                # 印出訓練結果\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                # 重置 batch 追蹤變數\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        # 計算全部訓練資料的平均損失值\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # save the model\n",
    "        model_dir = 'gdrive/MyDrive/DL_NLP_marathon/model/Day31_bert/'\n",
    "        torch.save(model.state_dict(), model_dir + 'day31-epoch-{}.pt'.format(epoch_i+1))\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        if evaluation == True:\n",
    "            # 每個 epoch 訓練完畢後, 在驗證集上檢驗模型的表現\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            time_elapsed = time.time() - t0_epoch          \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    # 將模型切換到評量模式 : 此模式下暫停使用 dropout 層\n",
    "    model.eval()\n",
    "    # 紀錄評量函數\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # 驗證資料的每個 batch \n",
    "    for batch in val_dataloader:\n",
    "        # 將所有 batch 資料載入 GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        # 計算 logit 值\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "        # 計算損失值\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        # 取得預測值\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        # 計算 accuracy 數值\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    # 計算全部驗證資料的平均損失值與平均 accuracy\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkh_UvM2XkzU"
   },
   "source": [
    "# 執行跑參 : 依機器的計算能力調整 epoch 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399435,
     "status": "ok",
     "timestamp": 1622122205737,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "F2pkguY7XkzU",
    "outputId": "ed50fb51-5eb6-4d9e-fe2d-50fb91351b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.667906   |     -      |     -     |   15.31  \n",
      "   1    |   40    |   0.577205   |     -      |     -     |   14.72  \n",
      "   1    |   60    |   0.542282   |     -      |     -     |   15.00  \n",
      "   1    |   80    |   0.487930   |     -      |     -     |   15.19  \n",
      "   1    |   100   |   0.436168   |     -      |     -     |   15.41  \n",
      "   1    |   120   |   0.398120   |     -      |     -     |   15.60  \n",
      "   1    |   140   |   0.566869   |     -      |     -     |   15.79  \n",
      "   1    |   160   |   0.380368   |     -      |     -     |   15.72  \n",
      "   1    |   180   |   0.530444   |     -      |     -     |   15.66  \n",
      "   1    |   200   |   0.573941   |     -      |     -     |   15.65  \n",
      "   1    |   220   |   0.405523   |     -      |     -     |   15.67  \n",
      "   1    |   240   |   0.596214   |     -      |     -     |   15.65  \n",
      "   1    |   260   |   0.432015   |     -      |     -     |   15.67  \n",
      "   1    |   280   |   0.531258   |     -      |     -     |   15.67  \n",
      "   1    |   300   |   0.427410   |     -      |     -     |   15.68  \n",
      "   1    |   320   |   0.478046   |     -      |     -     |   15.69  \n",
      "   1    |   340   |   0.489731   |     -      |     -     |   15.70  \n",
      "   1    |   360   |   0.416764   |     -      |     -     |   15.70  \n",
      "   1    |   380   |   0.400595   |     -      |     -     |   15.68  \n",
      "   1    |   400   |   0.394697   |     -      |     -     |   15.67  \n",
      "   1    |   420   |   0.376027   |     -      |     -     |   15.68  \n",
      "   1    |   440   |   0.509019   |     -      |     -     |   15.69  \n",
      "   1    |   460   |   0.400645   |     -      |     -     |   15.68  \n",
      "   1    |   480   |   0.529419   |     -      |     -     |   15.68  \n",
      "   1    |   500   |   0.500277   |     -      |     -     |   15.71  \n",
      "   1    |   520   |   0.425933   |     -      |     -     |   15.70  \n",
      "   1    |   540   |   0.476411   |     -      |     -     |   15.71  \n",
      "   1    |   560   |   0.548870   |     -      |     -     |   15.70  \n",
      "   1    |   580   |   0.438794   |     -      |     -     |   15.73  \n",
      "   1    |   600   |   0.434112   |     -      |     -     |   15.72  \n",
      "   1    |   620   |   0.466950   |     -      |     -     |   15.71  \n",
      "   1    |   640   |   0.479771   |     -      |     -     |   15.71  \n",
      "   1    |   660   |   0.405035   |     -      |     -     |   15.71  \n",
      "   1    |   680   |   0.492465   |     -      |     -     |   15.71  \n",
      "   1    |   700   |   0.416633   |     -      |     -     |   15.71  \n",
      "   1    |   720   |   0.535174   |     -      |     -     |   15.72  \n",
      "   1    |   740   |   0.406482   |     -      |     -     |   15.71  \n",
      "   1    |   760   |   0.545271   |     -      |     -     |   15.70  \n",
      "   1    |   780   |   0.468929   |     -      |     -     |   15.71  \n",
      "   1    |   800   |   0.430864   |     -      |     -     |   15.71  \n",
      "   1    |   820   |   0.422350   |     -      |     -     |   15.71  \n",
      "   1    |   840   |   0.469319   |     -      |     -     |   15.72  \n",
      "   1    |   856   |   0.411928   |     -      |     -     |   12.11  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.473166   |  0.419976  |   82.29   |  697.55  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.292722   |     -      |     -     |   16.49  \n",
      "   2    |   40    |   0.313960   |     -      |     -     |   15.73  \n",
      "   2    |   60    |   0.467524   |     -      |     -     |   15.74  \n",
      "   2    |   80    |   0.308438   |     -      |     -     |   15.72  \n",
      "   2    |   100   |   0.213750   |     -      |     -     |   15.72  \n",
      "   2    |   120   |   0.352999   |     -      |     -     |   15.72  \n",
      "   2    |   140   |   0.353839   |     -      |     -     |   15.69  \n",
      "   2    |   160   |   0.364062   |     -      |     -     |   15.69  \n",
      "   2    |   180   |   0.329117   |     -      |     -     |   15.67  \n",
      "   2    |   200   |   0.298190   |     -      |     -     |   15.69  \n",
      "   2    |   220   |   0.255169   |     -      |     -     |   15.65  \n",
      "   2    |   240   |   0.326519   |     -      |     -     |   15.67  \n",
      "   2    |   260   |   0.419742   |     -      |     -     |   15.68  \n",
      "   2    |   280   |   0.335534   |     -      |     -     |   15.66  \n",
      "   2    |   300   |   0.346174   |     -      |     -     |   15.65  \n",
      "   2    |   320   |   0.340252   |     -      |     -     |   15.66  \n",
      "   2    |   340   |   0.218359   |     -      |     -     |   15.65  \n",
      "   2    |   360   |   0.482805   |     -      |     -     |   15.64  \n",
      "   2    |   380   |   0.263286   |     -      |     -     |   15.64  \n",
      "   2    |   400   |   0.328844   |     -      |     -     |   15.65  \n",
      "   2    |   420   |   0.321224   |     -      |     -     |   15.66  \n",
      "   2    |   440   |   0.335534   |     -      |     -     |   15.67  \n",
      "   2    |   460   |   0.266733   |     -      |     -     |   15.67  \n",
      "   2    |   480   |   0.373181   |     -      |     -     |   15.67  \n",
      "   2    |   500   |   0.458280   |     -      |     -     |   15.70  \n",
      "   2    |   520   |   0.325823   |     -      |     -     |   15.68  \n",
      "   2    |   540   |   0.262029   |     -      |     -     |   15.68  \n",
      "   2    |   560   |   0.303264   |     -      |     -     |   15.67  \n",
      "   2    |   580   |   0.271249   |     -      |     -     |   15.69  \n",
      "   2    |   600   |   0.281935   |     -      |     -     |   15.68  \n",
      "   2    |   620   |   0.413412   |     -      |     -     |   15.69  \n",
      "   2    |   640   |   0.324177   |     -      |     -     |   15.66  \n",
      "   2    |   660   |   0.392111   |     -      |     -     |   15.66  \n",
      "   2    |   680   |   0.425852   |     -      |     -     |   15.65  \n",
      "   2    |   700   |   0.430778   |     -      |     -     |   15.67  \n",
      "   2    |   720   |   0.291048   |     -      |     -     |   15.64  \n",
      "   2    |   740   |   0.300112   |     -      |     -     |   15.65  \n",
      "   2    |   760   |   0.311770   |     -      |     -     |   15.65  \n",
      "   2    |   780   |   0.294814   |     -      |     -     |   15.67  \n",
      "   2    |   800   |   0.432557   |     -      |     -     |   15.68  \n",
      "   2    |   820   |   0.240082   |     -      |     -     |   15.66  \n",
      "   2    |   840   |   0.287892   |     -      |     -     |   15.66  \n",
      "   2    |   856   |   0.275365   |     -      |     -     |   12.08  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.331157   |  0.510688  |   82.55   |  700.02  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42) # 設定隨機種子\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiPQAR9G9C8h"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dir = 'model/'\n",
    "torch.save(bert_classifier.state_dict(), model_dir + 'final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7350,
     "status": "ok",
     "timestamp": 1622105683924,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "4C5K7p6g-tV_",
    "outputId": "b24001b9-748e-4666-87cf-8d8af328e157"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the previous trained model\n",
    "set_seed(42) # 設定隨機種子\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "model_dir = 'model/'\n",
    "bert_classifier.load_state_dict(torch.load(model_dir + 'final.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaxNnaM4XkzU"
   },
   "source": [
    "# 繪製 ROC_AUC 圖形 : 藉此觀察預測效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1622122425169,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "phdGa5X5XkzU"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    # 將模型切換到評量模式 : 此模式下暫停使用 dropout 層\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "\n",
    "    # 測試資料的每個 batch \n",
    "    for batch in test_dataloader:\n",
    "        # 將所有 batch 資料載入 GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        # 計算機率\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    # 將每個 batch 的 logit 值連結起來\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    # 使用 softmax 計算機率\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1622122427160,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "2nK6w-NyXkzV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')       \n",
    "    # 取得測試集的 accuracy 值\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')    \n",
    "    # 繪製 ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "executionInfo": {
     "elapsed": 27646,
     "status": "ok",
     "timestamp": 1622122456843,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "r5Rx4KwpXkzV",
    "outputId": "e170b429-295d-404e-ff79-3287dcccaf16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8784\n",
      "Accuracy: 82.41%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在測試集上計算預測機率\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "# 評價 Bert 分類器\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3FtqgxMXkzW"
   },
   "source": [
    "# 重新以完整資料訓練模型, 並對要預測的資料進行輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1500989,
     "status": "ok",
     "timestamp": 1622120053844,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "B0KT0n72XkzW",
    "outputId": "8f19c838-1f87-4af6-f1c5-90ee57de29ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.675277   |     -      |     -     |   14.89  \n",
      "   1    |   40    |   0.573323   |     -      |     -     |   14.30  \n",
      "   1    |   60    |   0.522759   |     -      |     -     |   14.56  \n",
      "   1    |   80    |   0.481142   |     -      |     -     |   14.75  \n",
      "   1    |   100   |   0.461739   |     -      |     -     |   15.05  \n",
      "   1    |   120   |   0.573266   |     -      |     -     |   15.25  \n",
      "   1    |   140   |   0.515599   |     -      |     -     |   15.56  \n",
      "   1    |   160   |   0.553770   |     -      |     -     |   15.78  \n",
      "   1    |   180   |   0.483958   |     -      |     -     |   15.83  \n",
      "   1    |   200   |   0.560076   |     -      |     -     |   15.70  \n",
      "   1    |   220   |   0.494516   |     -      |     -     |   15.58  \n",
      "   1    |   240   |   0.618852   |     -      |     -     |   15.55  \n",
      "   1    |   260   |   0.550959   |     -      |     -     |   15.60  \n",
      "   1    |   280   |   0.523858   |     -      |     -     |   15.69  \n",
      "   1    |   300   |   0.513944   |     -      |     -     |   15.75  \n",
      "   1    |   320   |   0.519803   |     -      |     -     |   15.68  \n",
      "   1    |   340   |   0.405549   |     -      |     -     |   15.64  \n",
      "   1    |   360   |   0.599073   |     -      |     -     |   15.63  \n",
      "   1    |   380   |   0.406240   |     -      |     -     |   15.63  \n",
      "   1    |   400   |   0.529630   |     -      |     -     |   15.63  \n",
      "   1    |   420   |   0.543386   |     -      |     -     |   15.66  \n",
      "   1    |   440   |   0.351351   |     -      |     -     |   15.66  \n",
      "   1    |   460   |   0.695633   |     -      |     -     |   15.67  \n",
      "   1    |   480   |   0.409355   |     -      |     -     |   15.67  \n",
      "   1    |   500   |   0.566318   |     -      |     -     |   15.68  \n",
      "   1    |   520   |   0.390494   |     -      |     -     |   15.68  \n",
      "   1    |   540   |   0.450416   |     -      |     -     |   15.70  \n",
      "   1    |   560   |   0.479595   |     -      |     -     |   15.70  \n",
      "   1    |   600   |   0.433898   |     -      |     -     |   15.71  \n",
      "   1    |   620   |   0.413570   |     -      |     -     |   15.73  \n",
      "   1    |   640   |   0.372675   |     -      |     -     |   15.72  \n",
      "   1    |   660   |   0.545904   |     -      |     -     |   15.72  \n",
      "   1    |   680   |   0.429733   |     -      |     -     |   15.71  \n",
      "   1    |   700   |   0.426558   |     -      |     -     |   15.77  \n",
      "   1    |   720   |   0.500341   |     -      |     -     |   15.72  \n",
      "   1    |   740   |   0.332542   |     -      |     -     |   15.67  \n",
      "   1    |   760   |   0.417562   |     -      |     -     |   15.66  \n",
      "   1    |   780   |   0.493480   |     -      |     -     |   15.65  \n",
      "   1    |   800   |   0.480166   |     -      |     -     |   15.66  \n",
      "   1    |   820   |   0.429898   |     -      |     -     |   15.65  \n",
      "   1    |   840   |   0.414057   |     -      |     -     |   15.68  \n",
      "   1    |   860   |   0.479096   |     -      |     -     |   15.68  \n",
      "   1    |   880   |   0.454563   |     -      |     -     |   15.66  \n",
      "   1    |   900   |   0.527494   |     -      |     -     |   15.69  \n",
      "   1    |   920   |   0.508706   |     -      |     -     |   15.69  \n",
      "   1    |   940   |   0.488752   |     -      |     -     |   15.69  \n",
      "   1    |   951   |   0.500740   |     -      |     -     |   8.37   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.294015   |     -      |     -     |   16.39  \n",
      "   2    |   40    |   0.356097   |     -      |     -     |   16.08  \n",
      "   2    |   60    |   0.318872   |     -      |     -     |   15.98  \n",
      "   2    |   80    |   0.327416   |     -      |     -     |   15.67  \n",
      "   2    |   100   |   0.379511   |     -      |     -     |   15.47  \n",
      "   2    |   120   |   0.382239   |     -      |     -     |   15.51  \n",
      "   2    |   140   |   0.304253   |     -      |     -     |   15.65  \n",
      "   2    |   160   |   0.376206   |     -      |     -     |   15.79  \n",
      "   2    |   180   |   0.437780   |     -      |     -     |   15.77  \n",
      "   2    |   200   |   0.300455   |     -      |     -     |   15.65  \n",
      "   2    |   220   |   0.435704   |     -      |     -     |   15.60  \n",
      "   2    |   240   |   0.375253   |     -      |     -     |   15.62  \n",
      "   2    |   260   |   0.374599   |     -      |     -     |   15.66  \n",
      "   2    |   280   |   0.359462   |     -      |     -     |   15.67  \n",
      "   2    |   300   |   0.294005   |     -      |     -     |   15.69  \n",
      "   2    |   320   |   0.298345   |     -      |     -     |   15.66  \n",
      "   2    |   340   |   0.403352   |     -      |     -     |   15.69  \n",
      "   2    |   360   |   0.486119   |     -      |     -     |   15.66  \n",
      "   2    |   380   |   0.431140   |     -      |     -     |   15.68  \n",
      "   2    |   400   |   0.326851   |     -      |     -     |   15.66  \n",
      "   2    |   420   |   0.291241   |     -      |     -     |   15.65  \n",
      "   2    |   440   |   0.237161   |     -      |     -     |   15.66  \n",
      "   2    |   460   |   0.354178   |     -      |     -     |   15.67  \n",
      "   2    |   480   |   0.395155   |     -      |     -     |   15.64  \n",
      "   2    |   500   |   0.336585   |     -      |     -     |   15.64  \n",
      "   2    |   520   |   0.321304   |     -      |     -     |   15.64  \n",
      "   2    |   540   |   0.426044   |     -      |     -     |   15.63  \n",
      "   2    |   560   |   0.290978   |     -      |     -     |   15.62  \n",
      "   2    |   580   |   0.383562   |     -      |     -     |   15.64  \n",
      "   2    |   600   |   0.501040   |     -      |     -     |   15.62  \n",
      "   2    |   620   |   0.348336   |     -      |     -     |   15.64  \n",
      "   2    |   640   |   0.352918   |     -      |     -     |   15.63  \n",
      "   2    |   660   |   0.344822   |     -      |     -     |   15.65  \n",
      "   2    |   680   |   0.371058   |     -      |     -     |   15.61  \n",
      "   2    |   700   |   0.347182   |     -      |     -     |   15.63  \n",
      "   2    |   720   |   0.380702   |     -      |     -     |   15.65  \n",
      "   2    |   740   |   0.380442   |     -      |     -     |   15.64  \n",
      "   2    |   760   |   0.372795   |     -      |     -     |   15.62  \n",
      "   2    |   780   |   0.428339   |     -      |     -     |   15.63  \n",
      "   2    |   800   |   0.315991   |     -      |     -     |   15.61  \n",
      "   2    |   820   |   0.378915   |     -      |     -     |   15.63  \n",
      "   2    |   840   |   0.412153   |     -      |     -     |   15.62  \n",
      "   2    |   860   |   0.549247   |     -      |     -     |   15.64  \n",
      "   2    |   880   |   0.417743   |     -      |     -     |   15.62  \n",
      "   2    |   900   |   0.310052   |     -      |     -     |   15.62  \n",
      "   2    |   920   |   0.239549   |     -      |     -     |   15.61  \n",
      "   2    |   940   |   0.372756   |     -      |     -     |   15.63  \n",
      "   2    |   951   |   0.404792   |     -      |     -     |   8.33   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 連結訓練集與驗證集\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=8)\n",
    "\n",
    "# 在完整的訓練資料上重新訓練 Bert 分類器\n",
    "set_seed(42)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, full_train_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1622120111627,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "42F9JoIzXkzX",
    "outputId": "79fcb720-825f-495c-9afb-f65549e04c39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>8051</td>\n",
       "      <td>Refugees as citizens - The Hindu http://t.co/G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>425</td>\n",
       "      <td>@5SOStag honestly he could say an apocalypse i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1330</td>\n",
       "      <td>If you bored as shit don't nobody fuck wit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>663</td>\n",
       "      <td>@RealTwanBrown Yesterday I Had A Heat Attack ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>2930</td>\n",
       "      <td>The Devil Wears Prada is still one of my favou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "2406  8051  Refugees as citizens - The Hindu http://t.co/G...\n",
       "134    425  @5SOStag honestly he could say an apocalypse i...\n",
       "411   1330  If you bored as shit don't nobody fuck wit you...\n",
       "203    663  @RealTwanBrown Yesterday I Had A Heat Attack ?...\n",
       "889   2930  The Devil Wears Prada is still one of my favou..."
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2392,
     "status": "ok",
     "timestamp": 1622120146112,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "8imzaomzXkzY",
    "outputId": "73ce2adb-4899-4fd5-eed5-9f910b2165c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# 在測試集推文上執行 `preprocessing_for_bert` 函數\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_df['text'])\n",
    "# 宣告測試集的 DataLoader\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115110,
     "status": "ok",
     "timestamp": 1622120263693,
     "user": {
      "displayName": "chi-fen Liao",
      "photoUrl": "",
      "userId": "16137842954465597187"
     },
     "user_tz": -480
    },
    "id": "AM4o-sxDXkzZ",
    "outputId": "3d43df52-fbe2-449b-e2d6-088bc507c647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets predicted non-negative:  1106\n"
     ]
    }
   ],
   "source": [
    "# 在測試資料上計算最終預測機率\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "# 將機率值轉為預測(超過門檻的預測為 1, 否則為 0)\n",
    "threshold = 0.9\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "# 顯示被判定為\n",
    "print(\"Number of tweets predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtjcf_WwXkza"
   },
   "outputs": [],
   "source": [
    "# 生成提交擋\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "submission['target'] = preds\n",
    "submission.to_csv('submission_FineTuneBert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day31_作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
